@inproceedings{Abdallah2016,
   author = {S Abdallah and E Benetos and N Gold and S Hargreaves and T Weyde and D Wolff},
   journal = {24th European Signal Processing Conference},
   month = {8},
   pages = {1118-1122},
   publisher = {EURASIP},
   title = {Digital Music Lab: A Framework for Analysing Big Music Data},
   year = {2016},
}
@article{Agres2016,
   abstract = {© 2016 ACM.The field of computational creativity, including musical metacreation, strives to develop artificial systems that are capable of demonstrating creative behavior or producing creative artefacts. But the claim of creativity is often assessed, subjectively only on the part of the researcher and not objectively at all. This article provides theoretical motivation for more systematic evaluation of musical metacreation and computationally creative systems and presents an overview of current methods used to assess human and machine creativity that may be adapted for this purpose. In order to highlight the need for a varied set of evaluation tools, a distinction is drawn among three types of creative systems: those that are purely generative, those that contain internal or external feedback, and those that are capable of reflection and self-reflection. To address the evaluation of each of these aspects, concrete examples of methods and techniques are suggested to help researchers (1) evaluate their systems' creative process and generated artefacts, and test their impact on the perceptual, cognitive, and affective states of the audience, and (2) build mechanisms for reflection into the creative system, including models of human perception and cognition, to endow creative systems with internal evaluative mechanisms to drive self-reflective processes. The first type of evaluation can be considered external to the creative system and may be employed by the researcher to both better understand the efficacy of their system and its impact and to incorporate feedback into the system. Here we take the stance that understanding human creativity can lend insight to computational approaches, and knowledge of how humans perceive creative systems and their output can be incorporated into artificial agents as feedback to provide a sense of how a creation will impact the audience. The second type centers around internal evaluation, in which the system is able to reason about its own behavior and generated output. We argue that creative behavior cannot occur without feedback and reflection by the creative/metacreative system itself. More rigorous empirical testing will allow computational and metacreative systems to become more creative by definition and can be used to demonstrate the impact and novelty of particular approaches.},
   author = {K Agres and J Forth and G A Wiggins},
   doi = {10.1145/2967506},
   issn = {1544-3574},
   issue = {3},
   journal = {Computers in Entertainment},
   month = {12},
   title = {Evaluation of musical creativity and musical metacreation systems},
   volume = {14},
   year = {2016},
}
@inproceedings{Agres2016b,
   abstract = {© 2016, CEUR-WS. All rights reserved.In this paper, we present a novel application of a computational model of word meaning to capture human judgments of the linguistic properties of metaphoricity, familiarity, and meaningfulness. We present data gathered from human subjects regarding their ratings of these properties over a set of word pairs specifically designed to exhibit varying degrees of metaphoricity. We then investigate whether these properties can be measured in terms of geometric features of a model of distributional lexical semantics. We compare the performance of two models, our own Concept Discovery Model which dynamically constructs context-sensitive subspaces, and a state-of-the-art static distributional semantic model, and find that our dynamic model performs significantly better in its measurement of metaphoricity.},
   author = {K R Agres and S McGregor and K Rataj and M Purver and G A Wiggins},
   issn = {1613-0073},
   journal = {CEUR Workshop Proceedings},
   month = {1},
   title = {Modeling metaphor perception with distributional semantics vector space models},
   volume = {1767},
   year = {2016},
}
@inproceedings{Agres2016c,
   author = {K Agres and D HERREMANS and L Bigo and D Conklin},
   journal = {14th International Conference for Music Perception and Cognition (ICMPC)},
   month = {6},
   pages = {280-282},
   title = {The Effect of Repetitive Structure on Enjoyment in Uplifting Trance Music},
   year = {2016},
}
@inproceedings{Allik2016,
   author = {A Allik and G Fazekas and M Sandler},
   doi = {10.1007/978-3-319-46547-0_1},
   isbn = {978-3-319-46546-3},
   issn = {0302-9743},
   journal = {SEMANTIC WEB - ISWC 2016, PT II},
   pages = {3-11},
   title = {Ontological Representation of Audio Features},
   volume = {9982},
   url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000389086600001&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a},
   year = {2016},
}
@inproceedings{alvarado2016,
   abstract = {Real music signals are highly variable, yet they have strong statistical
structure. Prior information about the underlying physical mechanisms
by which sounds are generated and rules by which complex sound struc-
ture is constructed (notes, chords, a complete musical score), can be nat-
urally unified using Bayesian modelling techniques. Typically algorithms
for Automatic Music Transcription independently carry out individual
tasks such as multiple-F0 detection and beat tracking. The challenge
remains to perform joint estimation of all parameters. We present a Ba-
yesian approach for modelling music audio, and content analysis. The
proposed methodology based on Gaussian processes seeks joint estima-
tion of multiple music concepts by incorporating into the kernel prior
information about non-stationary behaviour, dynamics, and rich spectral
content present in the modelled music signal. We illustrate the benefits
of this approach via two tasks: pitch estimation, and inferring missing
segments in a polyphonic audio recording.},
   author = {P A Alvarado Duran and D F STOWELL},
   journal = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
   title = {Gaussian Processes for Music Audio Modelling and Content Analysis},
   year = {2016},
}
@article{Barascud2016,
   abstract = {We use behavioral methods, magnetoencephalography, and functional MRI to investigate how human listeners discover temporal patterns and statistical regularities in complex sound sequences. Sensitivity to patterns is fundamental to sensory processing, in particular in the auditory system, because most auditory signals only have meaning as successions over time. Previous evidence suggests that the brain is tuned to the statistics of sensory stimulation. However, the process through which this arises has been elusive. We demonstrate that listeners are remarkably sensitive to the emergence of complex patterns within rapidly evolving sound sequences, performing on par with an ideal observer model. Brain responses reveal online processes of evidence accumulation–dynamic changes in tonic activity precisely correlate with the expected precision or predictability of ongoing auditory input–both in terms of deterministic (first-order) structure and the entropy of random sequences. Source analysis demonstrates an interaction between primary auditory cortex, hippocampus, and inferior frontal gyrus in the process of discovering the regularity within the ongoing sound sequence. The results are consistent with precision based predictive coding accounts of perceptual inference and provide compelling neurophysiological evidence of the brain's capacity to encode high-order temporal structure in sensory signals.},
   author = {N Barascud and M T Pearce and T D Griffiths and K J Friston and M Chait},
   doi = {10.1073/pnas.1508523113},
   issue = {5},
   journal = {Proc Natl Acad Sci U S A},
   month = {2},
   pages = {E616–E625},
   title = {Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns.},
   volume = {113},
   url = {https://www.ncbi.nlm.nih.gov/pubmed/26787854},
   year = {2016},
}
@inproceedings{BARTHET2016,
   abstract = {We discuss several state-of-the-art systems that propose
new paradigms and user workflows for music composition,
production, performance, and listening. We focus on a selection
of systems that exploit recent advances in semantic
and affective computing, music information retrieval (MIR)
and semantic web, as well as insights from fields such as
mobile computing and information visualisation. These systems
offer the potential to provide transformative experiences
for users, which is manifested in creativity, engagement,
efficiency, discovery and affect.},
   author = {M BARTHET and F Thalmann and G Fazekas and M Sandler and G Wiggins},
   journal = {ACM Conference on Human Factors in Computing Systems (CHI), Workshop on Music and HCI},
   month = {5},
   title = {Crossroads: Interactive Music Systems Transforming Performance, Production and Listening},
   year = {2016},
}
@inproceedings{BENETOS2016,
   abstract = {This work presents a probabilistic latent component analysis (PLCA) method applied to automatic music transcription of a cappella performances of vocal quartets. A variable-Q transform (VQT) representation of the audio spectrogram is factorised with the help of a 6-dimensional tensor. Preliminary experiments have shown promising music transcription results when applied to audio recordings of Bach Chorales and Barbershop music.},
   author = {E BENETOS and R Schramm},
   journal = {DMRN+11: Digital Music Research Network Workshop Proceedings 2016},
   month = {12},
   publisher = {Centre for Digital Music, Queen Mary University of London},
   title = {Automatic Transcription of Vocal Quartets},
   url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/19345},
   year = {2016},
}
@inproceedings{Benetos2016b,
   abstract = {In this paper, a system for overlapping acoustic event detection is proposed, which models the temporal evolution of sound events. The system is based on probabilistic latent component analysis, supporting the use of a sound event dictionary where each exemplar consists of a succession of spectral templates. The temporal succession of the templates is controlled through event class-wise Hidden Markov Models (HMMs). As input time/frequency representation, the Equivalent Rectangular Bandwidth (ERB) spectrogram is used. Experiments are carried out on polyphonic datasets of office sounds generated using an acoustic scene simulator, as well as real and synthesized monophonic datasets for comparative purposes. Results show that the proposed system outperforms several state-of-the-art methods for overlapping acoustic event detection on the same task, using both frame-based and event-based metrics, and is robust to varying event density and noise levels.},
   author = {E Benetos and G Lafay and M Lagrange and M D Plumbley},
   city = {Emmanouil Benetos, Queen Mary University of London, School of Electronic Engineering and Computer Science, Mile End Road, London, E1 4NS, United Kingdom},
   doi = {10.1109/ICASSP.2016.7472919},
   journal = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
   month = {3},
   pages = {6450-6454},
   publisher = {IEEE},
   title = {Detection of overlapping acoustic events using a temporally-constrained probabilistic model},
   url = {http://www.eecs.qmul.ac.uk/~emmanouilb/index.html},
   year = {2016},
}
@inproceedings{Cheng2016,
   abstract = {We demonstrate that piano transcription performance for a known piano can be improved by explicitly modelling piano acoustical features. The proposed method is based on non-negative matrix factorisation, with the following three refinements: (1) introduction of attack and harmonic decay components; (2) use of a spike-shaped note activation that is shared by these components; (3) modelling the harmonic decay with an exponential function. Transcription is performed in a supervised way, with the training and test datasets produced by the same piano. First we train parameters for the attack and decay components on isolated notes, then update only the note activations for transcription. Experiments show that the proposed model achieves 82% on note-wise and 79% on frame-wise F-measures on the ‘ENSTDkCl’ subset of the MAPS database, outperforming the current published state of the art.},
   author = {T Cheng and M Mauch and E Benetos and S Dixon},
   journal = {17th International Society for Music Information Retrieval Conference},
   month = {8},
   pages = {584-590},
   publisher = {ISMIR},
   title = {An attack/decay model for piano transcription},
   url = {https://wp.nyu.edu/ismir2016/},
   year = {2016},
}
@article{Chew2016,
   author = {E Chew},
   doi = {10.1525/MP.2016.33.03.344},
   issn = {0730-7829},
   issue = {3},
   journal = {MUSIC PERCEPTION},
   month = {2},
   pages = {344-366},
   title = {PLAYING WITH THE EDGE: TIPPING POINTS AND THE ROLE OF TONALITY},
   volume = {33},
   url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000373749600007&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a},
   year = {2016},
}
@inproceedings{CHOI2016,
   abstract = {In this paper, we introduce new methods and discuss results of text-based LSTM (Long Short-Term Memory) networks for automatic music composition. The proposed network is designed to learn relationships within text documents that represent chord progressions and drum tracks in two case studies. In the experiments, word-RNNs (Recurrent Neural Networks) show good results for both cases, while character-based RNNs (char-RNNs) only succeed to learn chord progressions. The proposed system can be used for fully automatic composition or as semi-automatic systems that help humans to compose music by controlling a diversity parameter of the model.},
   author = {K CHOI and M sandler and G fazekas},
   city = {Keunwoo Choi, Queen Mary University of London, EECS, Peter Landin Building, CS.319, London, E1 4FZ, United Kingdom},
   journal = {Conference on Computer Simulation of Musical Creativity},
   month = {6},
   title = {Text-based LSTM networks for Automatic Music Composition},
   year = {2016},
}
@inproceedings{CHOURDAKIS2016,
   abstract = {Adaptive Digital Audio Effects are sound transformations controlled by features extracted from the sound itself. Artificial reverberation is used by sound engineers in the mixing process for a variety of technical and artistic reasons, including to give the perception that it was captured in a closed space. We propose a design of an adaptive digital audio effect for artificial reverberation that allows it to learn from the user in a supervised way. We perform feature selection and dimensionality reduction on features extracted from our training data set. Then a user provides examples of reverberation parameters for the training data. Finally, we train a set of classifiers and compare them using 10-fold cross validation to compare classification success ratios and mean squared errors. Tracks from the Open Multitrack Testbed are used in order to train and test our models.},
   author = {E T CHOURDAKIS and J D REISS},
   journal = {60th International Conference: DREAMS (Dereverberation and Reverberation of Audio, Music, and Speech)},
   month = {1},
   title = {Automatic Control of a Digital Reverberation Effect using Hybrid Models},
   year = {2016},
}
@inproceedings{Brecht2016,
   abstract = {The Open Multitrack Testbed is an online repository of multitrack audio accessible to the public, with rich metadata annotation, a semantic database and search functionality. Two years after it first went live, the dataset is the largest and most diverse available, and still growing. An overview of the available content, some prominent features, and example uses in the field of intelligent music production are dis- cussed.},
   author = {B M DE MAN and J D Reiss},
   journal = {http://c4dm.eecs.qmul.ac.uk/events/wimp2/},
   month = {9},
   title = {The Open Multitrack Testbed: Features, content and use cases},
   year = {2016},
}
@inproceedings{Brecht2016b,
   abstract = {The Web Audio Evaluation Tool is an open-source, browser- based framework for creating and conducting listening tests. It allows remote deployment, GUI-guided setup, and analysis in the browser. While currently being used for listening tests in various fields, it was initially developed specifically for the study of music production practices. In this work, we highlight some of the features that facilitate evaluation of such content.},
   author = {B M DE MAN and N Jillings and D Moffat and J D Reiss and R Stables},
   journal = {http://c4dm.eecs.qmul.ac.uk/events/wimp2/},
   month = {9},
   title = {Subjective comparison of music production practices using the Web Audio Evaluation Tool},
   year = {2016},
}
@article{Dean2016,
   author = {R T Dean and M T Pearce},
   doi = {10.18061/emr.v11i1.4900},
   issn = {1559-5749},
   issue = {1},
   journal = {Empirical Musicology Review},
   month = {7},
   pages = {27},
   title = {Algorithmically-generated Corpora that use Serial Compositional Principles Can Contribute to the Modeling of Sequential Pitch Structure in Non-tonal Music},
   volume = {11},
   year = {2016},
}
@inproceedings{Driedger2016,
   author = {J Driedger and S Balke and S Ewert and M Müller},
   city = {New York, USA},
   journal = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
   pages = {239-245},
   title = {Template-Based Vibrato Analysis in Music Signals},
   year = {2016},
}
@article{Ewert2016,
   abstract = {© 2016 IEEE.Given a musical audio recording, the goal of automatic music transcription is to determine a score-like representation of the piece underlying the recording. Despite significant interest within the research community, several studies have reported on a 'glass ceiling' effect, an apparent limit on the transcription accuracy that current methods seem incapable of overcoming. In this paper, we explore how much this effect can be mitigated by focusing on a specific instrument class and making use of additional information on the recording conditions available in studio or home recording scenarios. In particular, exploiting the availability of single note recordings for the instrument in use, we develop a novel signal model employing variable-length spectro-temporal patterns as its central building blocks - tailored for pitched percussive instruments such as the piano. Temporal dependencies between spectral templates are modeled, resembling characteristics of factorial scaled hidden Markov models (FS-HMM) and other methods combining nonnegative matrix factorization with Markov processes. In contrast to FS-HMMs, our parameter estimation is developed in a global, relaxed form within the extensible alternating direction method of multipliers framework, which enables the systematic combination of basic regularizers propagating sparsity and local stationarity in note activity with more complex regularizers imposing temporal semantics. The proposed method achieves an f -measure of 93-95% for note onsets on pieces recorded on a Yamaha Disklavier (MAPS DB).},
   author = {S Ewert and M Sandler},
   doi = {10.1109/TASLP.2016.2593801},
   issn = {2329-9290},
   issue = {11},
   journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
   month = {11},
   pages = {1983-1997},
   title = {Piano transcription in the studio using an extensible alternating directions framework},
   volume = {24},
   year = {2016},
}
@article{Ewert2016b,
   abstract = {In terms of terminology, “musical structure” has been used in several, different contexts. In one interpretation, musical structure is essentially equivalent to musical form, which can be considered as a genre or rather style specific definition of the expectation of how a piece is composed on a rather global level. Another interpretation of structure is closer to the corresponding mathematical notion, where structure yields properties and regularity.},
   author = {S Ewert},
   city = {Dagstuhl, Germany},
   doi = {10.4230/DagRep.6.2.147},
   editor = {M Müller and E Chew and J P Bello},
   issn = {2192-5283},
   issue = {2},
   journal = {Dagstuhl Reports (Computational Music Structure Analysis (Dagstuhl Seminar 16092))},
   month = {10},
   note = {urn: urn:nbn:de:0030-drops-61415},
   pages = {175-176},
   publisher = {Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik},
   title = {Representation of Musical Structure for a Computationally Feasible Integration with Audio-Based Methods},
   volume = {6},
   url = {http://drops.dagstuhl.de/opus/volltexte/2016/6141},
   year = {2016},
}
@inproceedings{Ewert2016c,
   author = {S Ewert and S Wang and M Müller and M Sandler},
   city = {New York, USA},
   journal = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
   pages = {30-36},
   title = {Score-Informed Identification of Missing and Extra Notes in Piano Recordings},
   year = {2016},
}
@article{FORTH2016,
   abstract = {We present a novel hypothetical account of entrainment in music and language, in context of the Information Dynamics of Thinking model, IDyOT. The extended model affords an alternative view of entrainment, and its companion term, pulse, from earlier accounts. The model is based on hiearchical, statistical prediction, modeling expectations of both what an event will be and when it will happen. As such,it constitutes a kind of predictive coding, with a particular novel hypothetical implementation. Here, we focus on the model’s mechanism for predicting when a perceptual event will happen, given an existing sequence of past events, which maybe musical or linguistic. We propose a range of tests to validate or falsify the model, at various different levels of abstraction, and argue that computational modelling in particular, and this model in particular, can offer a means of providing limited but useful evidence for evolutionary hypotheses.},
   author = {J FORTH and K AGRES and M R J PURVER and G A WIGGINS},
   doi = {10.3389/fpsyg.2016.01575},
   editor = {A RAVIGNANI},
   issn = {1664-1078},
   journal = {Frontiers in Psychology},
   month = {10},
   pages = {1575},
   publisher = {Frontiers Media},
   title = {Entraining IDyOT: timing in the information dynamics of thinking},
   volume = {7},
   url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01575/abstract},
   year = {2016},
}
@article{Gingras2016,
   abstract = {This research explored the relations between the predictability of musical structure, expressive timing in performance, and listeners' perceived musical tension. Studies analyzing the influence of expressive timing on listeners' affective responses have been constrained by the fact that, in most pieces, the notated durations limit performers' interpretive freedom. To circumvent this issue, we focused on the unmeasured prelude, a semi-improvisatory genre without notated durations. In Experiment 1, 12 professional harpsichordists recorded an unmeasured prelude on a harpsichord equipped with a MIDI console. Melodic expectation was assessed using a probabilistic model (IDyOM [Information Dynamics of Music]) whose expectations have been previously shown to match closely those of human listeners. Performance timing information was extracted from the MIDI data using a score-performance matching algorithm. Time-series analyses showed that, in a piece with unspecified note durations, the predictability of melodic structure measurably influenced tempo fluctuations in performance. In Experiment 2, another 10 harpsichordists, 20 nonharpsichordist musicians, and 20 nonmusicians listened to the recordings from Experiment 1 and rated the perceived tension continuously. Granger causality analyses were conducted to investigate predictive relations among melodic expectation, expressive timing, and perceived tension. Although melodic expectation, as modeled by IDyOM, modestly predicted perceived tension for all participant groups, neither of its components, information content or entropy, was Granger causal. In contrast, expressive timing was a strong predictor and was Granger causal. However, because melodic expectation was also predictive of expressive timing, our results outline a complete chain of influence from predictability of melodic structure via expressive performance timing to perceived musical tension. (PsycINFO Database Record},
   author = {B Gingras and M T Pearce and M Goodchild and R T Dean and G Wiggins and S McAdams},
   city = {Institute of Psychology, University of Innsbruck.},
   doi = {10.1037/xhp0000141},
   issn = {0096-1523},
   issue = {4},
   journal = {Journal of experimental psychology. Human perception and performance},
   month = {4},
   pages = {594-609},
   title = {Linking melodic expectation to expressive performance timing and perceived musical tension.},
   volume = {42},
   year = {2016},
}
@article{Hansen2016,
   author = {N C Hansen and P Vuust and M Pearce},
   doi = {10.1371/journal.pone.0163584},
   editor = {L Jaencke},
   issue = {10},
   journal = {PLOS ONE},
   month = {10},
   pages = {e0163584–e0163584},
   title = {"If You Have to Ask, You'll Never Know": Effects of Specialised Stylistic Expertise on Predictive Processing of Music},
   volume = {11},
   year = {2016},
}
@article{Hansen2016b,
   author = {N C Hansen and M Sadakata and M Pearce},
   doi = {10.1525/mp.2016.33.4.414},
   issn = {0730-7829},
   issue = {4},
   journal = {Music Perception: An Interdisciplinary Journal},
   month = {4},
   pages = {414-431},
   title = {Nonlinear Changes in the Rhythm of European Art Music: Quantitative Support for Historical Musicology},
   volume = {33},
   year = {2016},
}
@inproceedings{Hayes2016,
   abstract = {Our Open Symphony system reimagines the music
experience for a digital age, fostering alliances between
performer and audience and our digital selves. Open
Symphony enables live participatory music performance
where the audience actively engages in the music
creation process. This is made possible by using stateof-
the-art web technologies and data visualisation
techniques. Through collaborations with local
performers we will conduct a series of interactive music
performance revolutionizing the performance
experience both for performers and audiences. The
system throws open music-creating possibilities to
every participant and is a genuine novel way to
demonstrate the field of Human Computer Interaction
through computer-supported cooperative creation and
multimodal music and visual perception.},
   author = {K Hayes and M BARTHET and Y Wu and L Zhang and N Bryan-Kinns},
   doi = {10.1145/2851581.2889471},
   journal = {ACM Conference on Human Factors in Computing Systems (CHI): Interactivity},
   month = {5},
   title = {A Participatory Live Music Performance with the Open Symphony System},
   year = {2016},
}
@article{HEALEY2016,
   abstract = {Empirical evidence from dialogue, both corpus and experimental, highlights the importance of interaction in language use – and this raises some questions for Christiansen & Chater’s (C&C’s) proposals. We endorse C&C’s call for an integrated framework but argue that their emphasis on local, individual production and comprehension makes it difficult to accommodate the ubiquitous, interactive, and defeasible processes of clarification and repair in conversation.},
   author = {P G T HEALEY and C HOWES and J HOUGH and M R J PURVER},
   doi = {10.1017/S0140525X15000813},
   issn = {1469-1825},
   journal = {Behavioral and Brain Sciences},
   month = {1},
   pages = {e76–e76},
   publisher = {Cambridge University Press (CUP): STM Journals},
   title = {Better late than Now-or-Never: The case of interactive repair phenomena},
   volume = {39},
   url = {http://www.eecs.qmul.ac.uk/~mpurver/papers/healey-et-al16bbs.pdf},
   year = {2016},
}
@inproceedings{Heinrichs2016,
   author = {C Heinrichs and A McPherson and ACM},
   doi = {10.1145/2839462.2854109},
   journal = {PROCEEDINGS OF THE TENTH ANNIVERSARY CONFERENCE ON TANGIBLE EMBEDDED AND EMBODIED INTERACTION (TEI16)},
   pages = {697-700},
   title = {Performance-Led Design of Computationally Generated Audio for Interactive Applications},
   url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000390588700100&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a},
   year = {2016},
}
@inproceedings{HERREMANS2016,
   author = {D HERREMANS and E Chew},
   journal = {IEEE TENCON},
   month = {11},
   title = {MorpheuS: Automatic music generation with recurrent pattern constraints and tension profiles},
   year = {2016},
}
@inproceedings{HERREMANS2016b,
   author = {D HERREMANS and E Chew},
   journal = {Second International Conference on Technologies for Music Notation and Representation},
   month = {5},
   pages = {8-18},
   title = {Tension ribbons: Quantifying and visualising tonal tension.},
   volume = {2},
   year = {2016},
}
@inproceedings{Holzapfel2016,
   abstract = {In this paper, we present a new corpus for research in computational ethnomusicology and automatic music transcription, consisting of traditional dance tunes from Crete. This rich dataset includes audio recordings, scores transcribed by ethnomusicologists and aligned to the audio performances, and meter annotations. A second contribution of this paper is the creation of an automatic music transcription system able to support the detection of multiple pitches produced by lyra (a bowed string instrument). Furthermore, the transcription system is able to cope with deviations from standard tuning, and provides temporally quantized notes by combining the output of the multi-pitch detection stage with a state-of-the-art meter tracking algorithm. Experiments carried out for note tracking using 25ms onset tolerance reach 41.1% using information from the multi-pitch detection stage only, 54.6% when integrating beat information, and 57.9% when also supporting tuning estimation. The produced meter aligned transcriptions can be used to generate staff notation, a fact that increases the value of the system for studies in ethnomusicology.},
   author = {A Holzapfel and E Benetos},
   journal = {17th International Society for Music Information Retrieval Conference},
   month = {8},
   pages = {531-537},
   publisher = {ISMIR},
   title = {The Sousta corpus: Beat-informed automatic transcription of traditional dance tunes},
   url = {https://wp.nyu.edu/ismir2016/},
   year = {2016},
}
@inproceedings{Jack2016,
   abstract = {© 2016 Copyright held by the owner/author(s).When designing digital musical instruments the importance of low and consistent action-to-sound latency is widely accepted. This paper investigates the effects of latency (0-20ms) on instrument quality evaluation and performer interaction. We present findings from an experiment conducted with musicians who performed on an percussive digital musical instrument with variable amounts of latency. Three latency conditions were tested against a zero latency condition, 10ms, 20ms and 10ms ± 3ms jitter. The zero latency condition was significantly rated more positively than the 10ms with jitter and 20ms latency conditions in six quality measures, emphasising the importance of not only low, but stable latency in digital musical instruments. There was no significant difference in rating between the zero latency condition and 10ms condition. A quantitative analysis of timing accuracy in a metronome task under latency conditions showed no significant difference in mean synchronisation error. This suggests that the 20ms and 10ms with jitter latency conditions degrade subjective impressions of an instrument, but without significantly affecting the timing performance of our participants. These findings are discussed in terms of control intimacy and instrument transparency.},
   author = {R H Jack and T Stockman and A McPherson},
   doi = {10.1145/2986416.2986428},
   isbn = {9781450348225},
   journal = {ACM International Conference Proceeding Series},
   month = {10},
   pages = {116-123},
   title = {Effect of latency on performer interaction and subjective quality assessment of a digital musical instrument},
   volume = {04-06-October-2016},
   year = {2016},
}
@inproceedings{Jack2016b,
   author = {R Jack and T Stockman and A McPherson and ACM},
   doi = {10.1145/2839462.2839503},
   journal = {PROCEEDINGS OF THE TENTH ANNIVERSARY CONFERENCE ON TANGIBLE EMBEDDED AND EMBODIED INTERACTION (TEI16)},
   pages = {3-11},
   title = {Navigation of Pitch Space on a Digital Musical Instrument with Dynamic Tactile Feedback},
   url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000390588700003&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a},
   year = {2016},
}
@inproceedings{Jillings2016,
   author = {Nicholas Jillings and Brecht De Man and David Moffat and Joshua D Reiss},
   journal = {Proc. 2nd Web Audio Conference},
   month = {4},
   title = {Web Audio Evaluation Tool: A framework for subjective assessment of audio},
   year = {2016},
}

@article{Kosta2016,
   abstract = {Loudness variation is one of the foremost tools for expressivity in music performance. Loudness is frequently notated as dynamic markings such as "p" (piano, meaning soft) or "f" (forte, meaning loud). While dynamic markings in music scores are important indicators of how music pieces should be interpreted, their meaning is less straightforward than it may seem, and depends highly on the context in which they appear. In this article, we investigate the relationship between dynamic markings in the score and performed loudness by applying machine learning techniques – decision trees, support vector machines, artificial neural networks, and a k-nearest neighbor method – to the prediction of loudness levels corresponding to dynamic markings, and to the classification of dynamic markings given loudness values. The methods are applied to 44 recordings of performances of Chopin's Mazurkas, each by 8 pianists. The results show that loudness values and markings can be predicted relatively well when trained across recordings of the same piece, but fail dismally when trained across the pianist's recordings of other pieces, demonstrating that score features may trump individual style when modeling loudness choices. Evidence suggests that all the features chosen for the task are relevant, and analysis of the results reveals the forms (such as the return of the theme) and structures (such as dynamic-marking repetitions) that influence the predictability of loudness and markings. Modeling of loudness trends in expressive performance appears to be a delicate matter, and sometimes loudness expression can be a matter of the performer's idiosyncracy.},
   author = {K Kosta and R Ramirez and O F Bandtlow and E Chew},
   editor = {T Fiore},
   issn = {1745-9745},
   journal = {Journal of Mathematics and Music},
   month = {8},
   pages = {149-172},
   publisher = {Taylor &amp; Francis: STM, Behavioural Science and Public Health Titles},
   title = {Mapping between dynamic markings and performed loudness: A machine learning approach},
   volume = {10},
   year = {2016},
}
@article{Lafay2016,
   abstract = {This paper introduces a model for simulating environmental acoustic scenes that abstracts temporal structures from audio recordings. This model allows us to explicitly control key morphological aspects of the acoustic scene and to isolate their impact on the performance of the system under evaluation. Thus, more information can be gained on the behavior of an evaluated system, providing guidance for further improvements. To demonstrate its potential, this model is employed to evaluate the performance of nine state of the art sound event detection systems submitted to the IEEE DCASE 2013 Challenge. Results indicate that the proposed scheme is able to successfully build datasets useful for evaluating important aspects of the performance of sound event detection systems, such as their robustness to new recording conditions and to varying levels of background audio.},
   author = {G Lafay and M Lagrange and M Rossignol and E Benetos and A Roebel},
   doi = {10.1109/TASLP.2016.2587218},
   issue = {10},
   journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
   month = {10},
   pages = {1854-1864},
   publisher = {IEEE},
   title = {A morphological model for simulating acoustic scenes and its application to sound event detection},
   volume = {24},
   url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7503122},
   year = {2016},
}
@inproceedings{LIANG2016,
   abstract = {This paper presents the results of a study of piano pedaling techniques on the sustain pedal using a newly designed measurement system. This system is comprised of an optical sensor mounted in the pedal bearing block and the Bela platform for recording audio and sensor data. Using the gesture data collected from the system, the task of classifying these data by pedaling technique was undertaken using a Support Vector Machine (SVM)},
   author = {B LIANG and G fazekas and A mcpherson and M sandler},
   journal = {DMRN+11: Digital Music Research Network Workshop Proceedings 2016},
   month = {12},
   publisher = {Centre for Digital Music, Queen Mary University of London},
   title = {Classification of Piano Pedaling Techniques Using Gesture Data from a Non-Intrusive Measurement System},
   url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/19345},
   year = {2016},
}
@inproceedings{Mazzoni2016,
   abstract = {In this paper we present Moody, a haptic wearable prototype intended to enhance mood music in Film through haptic sensations. Moody is the design resulting from an exploratory study aimed at finding new ways to enrich emotions in film entertainment. The work is aimed at designing expressive haptic patterns to heighten suspense and excitement in those movie scenes where the music score amplifies what can't be shown, and that, is the mood.},
   author = {A Mazzoni and N Bryan-Kinns},
   doi = {10.1145/2908805.2908811},
   isbn = {9781450343152},
   journal = {DIS 2016 Companion - Proceedings of the 2016 ACM Conference on Designing Interactive Systems: Fuse},
   month = {6},
   pages = {21-24},
   title = {Moody: Haptic sensations to enhance mood in film music},
   year = {2016},
}
@inproceedings{McPherson2016,
   abstract = {The importance of low and consistent latency in interactive music systems is well-established. So how do commonly-used tools for creating digital musical instruments and other tangible interfaces perform in terms of latency from user action to sound output? This paper examines several common configurations where a microcontroller (e.g. Arduino) or wireless device communicates with computer-based sound generator (e.g. Max/MSP, Pd). We find that, perhaps surprisingly, almost none of the tested configurations meet generally-accepted guidelines for latency and jitter. To address this limitation, the paper presents a new embedded platform, Bela, which is capable of complex audio and sensor processing at submillisecond latency.},
   author = {A P McPherson and R H Jack and G Moro},
   journal = {Proceedings of the International Conference on New Interfaces for Musical Expression, Brisbane, Queensland, Australia, July 11-15, 2016},
   month = {7},
   note = {Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).},
   publisher = {Griffith University},
   title = {Action-Sound Latency: Are Our Tools Fast Enough?},
   url = {https://nime2016.wordpress.com/},
   year = {2016},
}
@inproceedings{MCPHERSON2016b,
   author = {A MCPHERSON and A Chamberlain and A Hazzard and S McGrath and S Benford},
   doi = {10.1145/2901790.2901831},
   isbn = {978-1-4503-4031-1},
   journal = {ACM Conference on Designing Interactive Systems},
   month = {6},
   title = {Designing for Exploratory Play with a Hackable Digital Musical Instrument},
   year = {2016},
}
@inproceedings{MEHRABI2016,
   author = {A MEHRABI and S Dixon and M Sandler},
   journal = {2nd AES Workshop on Intelligent Music Production},
   month = {9},
   title = {Towards a comprehensive dataset of vocal imitations of drum sounds},
   year = {2016},
}
@inproceedings{Mengual2016,
   author = {Lucas Mengual and David Moffat and Joshua D Reiss},
   journal = {Proc. Audio Engineering Society Conference: 61st International Conference: Audio for Games},
   title = {Modal Synthesis of Weapon Sounds},
   year = {2016},
}
@article{Metatla2016,
   author = {O Metatla and N Bryan-Kinns and T Stockman and F Martin},
   journal = {PeerJ Computer Science},
   month = {4},
   title = {Sonification of reference markers for auditory graphs: effects on non-visual estimation tasks},
   volume = {2},
   year = {2016},
}
@article{Metatla2016b,
   abstract = {© 2016, The Author(s).We examine how auditory displays, sonification and haptic interaction design can support visually impaired sound engineers, musicians and audio production specialists access to digital audio workstation. We describe a user-centred approach that incorporates various participatory design techniques to help make the design process accessible to this population of users. We also outline the audio-haptic designs that results from this process and reflect on the benefits and challenges that we encountered when applying these techniques in the context of designing support for audio editing.},
   author = {O Metatla and F Martin and A Parkinson and N Bryan-Kinns and T Stockman and A Tanaka},
   doi = {10.1007/s12193-016-0217-8},
   issn = {1783-8738},
   issue = {3},
   journal = {Journal on Multimodal User Interfaces},
   month = {9},
   pages = {247-258},
   publisher = {Springer Verlag (Germany)},
   title = {Audio-haptic interfaces for digital audio workstations: A participatory design approach},
   volume = {10},
   year = {2016},
}
@inproceedings{Metatla2016c,
   author = {O Metatla and N Correia and F Martin and N Bryan-Kinns and T Stockman},
   note = {proceedings: Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
},
   pages = {1055-1066},
   title = {Tap the ShapeTones: Exploring the Effects of Crossmodal Congruence in an Audio-Visual Interface},
   year = {2016},
}
@inproceedings{Moffat2016,
   author = {David Moffat and Joshua D Reiss},
   journal = {Proc. Audio Engineering Society Conference: 60th International Conference: DREAMS (Dereverberation and Reverberation of Audio, Music, and Speech)},
   title = {Implementation and Assessment of Joint Source Separation and Dereverberation},
   year = {2016},
}
@inproceedings{MORO2016,
   abstract = {Bela is an embedded platform for ultra-low latency audio and sensor processing. We present here the hardware and software features of Bela with particular focus on its integration with Pure Data. Sensor inputs on Bela are sampled at audio rate, which opens to the possibility of doing signal processing using Pure Data’s audio-rate objects.},
   author = {G MORO and A Bin and R H Jack and C Heinrichs and A P McPherson},
   journal = {International Conference on Live Interfaces},
   month = {6},
   note = {This hands-on workshop introduces participants to Bela, an embedded platform for ultra-low latency audio and sensor processing.},
   publisher = {University of Sussex},
   title = {Making High-Performance Embedded Instruments with Bela and Pure Data},
   url = {http://www.liveinterfaces.org/proceedings2016.html},
   year = {2016},
}
@inproceedings{hanlon2016,
   author = {K O'Hanlon and M B Sandler and IEEE},
   issn = {2076-1465},
   journal = {2016 24TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)},
   pages = {1237-1241},
   title = {COMPOSITIONAL CHROMA ESTIMATION USING POWERED EUCLIDEAN DISTANCE},
   url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000391891900236&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a},
   year = {2016},
}
@inproceedings{hanlon2016b,
   author = {K O'Hanlon and M B Sandler and IEEE},
   issn = {1520-6149},
   journal = {2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING PROCEEDINGS},
   pages = {4737-4741},
   title = {AN ITERATIVE HARD THRESHOLDING APPROACH TO l(0) SPARSE HELLINGER NMF},
   url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000388373404177&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a},
   year = {2016},
}
@inproceedings{Olowe2016,
   abstract = {We propose residUUm, an audiovisual performance tool that uses
sonification to orchestrate a particle system of shapes, as an attempt to build
an audiovisual user interface in which all the actions of a performer on a laptop
are intended to be explicitly interpreted by the audience. We propose two
approaches to performing with residUUm and discuss the methods utilized to
fulfill the promise of audience-visible interaction: mapping and performance
strategies applied to express audiovisual interactions with multilayered
sound-image relationships. The system received positive feedback from 34 audience
participants on aspects such as aesthetics and audiovisual integration, and we
identified further design challenges around performance clarity and strategy. We
discuss residUUm's development objectives, modes of interaction and the impact of
an audience-visible interface on the performer and observer. },
   author = {Ireti Olowe and Giulio Moro and Mathieu Barthet},
   city = {Brisbane, Australia},
   isbn = {978-1-925455-13-7},
   journal = {Proceedings of the International Conference on New Interfaces for Musical Expression},
   pages = {271-276},
   publisher = {Queensland Conservatorium Griffith University},
   title = {residUUm: user mapping and performance strategies for multilayered live audiovisual generation},
   volume = {16},
   url = {http://www.nime.org/proceedings/2016/nime2016_paper0053.pdf},
   year = {2016},
}
@article{Panteli2016,
   author = {Maria Panteli and Bruno Rocha and Niels Bogaards and Aline Honingh},
   doi = {10.1177/1029864916655596},
   journal = {Musicae Scientiae},
   publisher = {SAGE Publications},
   title = {A model for rhythm and timbre similarity in electronic dance music},
   year = {2016},
}
@inproceedings{Panteli2016b,
   abstract = {In this study we investigate computational methods for assessing music similarity in world music styles. We use state-of-the-art audio features to describe musical content in world music recordings. Our music collection is a subset of the Smithsonian Folkways Recordings with audio examples from 31 countries from around the world. Using supervised and unsupervised dimensionality reduction techniques we learn feature representations for music similarity. We evaluate how well music styles separate in this learned space with a classification experiment. We obtained moderate performance classifying the recordings by country. Analysis of misclassifications revealed cases of geographical or cultural proximity. We further evaluate the learned space by detecting outliers, i.e. identifying recordings that stand out in the collection. We use a data mining technique based on Mahalanobis distances to detect outliers and perform a listening experiment in the ‘odd one out’ style to evaluate our findings. We are able to detect, amongst others, recordings of non-musical content as outliers as well as music with distinct timbral and harmonic content. The listening experiment reveals moderate agreement between subjects’ ratings and our outlier estimation.},
   author = {M Panteli and E Benetos and S Dixon},
   journal = {17th International Society for Music Information Retrieval Conference},
   month = {8},
   pages = {538-544},
   publisher = {ISMIR},
   title = {Learning a feature space for similarity in world music},
   url = {http://www.eecs.qmul.ac.uk/~mp305/},
   year = {2016},
}
@inproceedings{Panteli2016c,
   author = {M Panteli and E Benetos and S Dixon},
   city = {Maria Panteli, Queen Mary University of London, School of Electronic Enginering and Computer Science, Mile End Road, London, E1 4NS, United Kingdom},
   journal = {Fourth International Conference on Analytical Approaches to World Music (AAWM 2016)},
   month = {6},
   title = {Automatic detection of outliers in world music collections},
   url = {http://www.eecs.qmul.ac.uk/~mp305/},
   year = {2016},
}
@article{Pearce2016,
   author = {M T Pearce and D W Zaidel and O Vartanian and M Skov and H Leder and A Chatterjee and M Nadal},
   doi = {10.1177/1745691615621274},
   issn = {1745-6916},
   issue = {2},
   journal = {Perspectives on psychological science : a journal of the Association for Psychological Science},
   month = {3},
   pages = {265-279},
   title = {Neuroaesthetics: The Cognitive Neuroscience of Aesthetic Experience.},
   volume = {11},
   year = {2016},
}
@book_section{PEARCE2016b,
   author = {M T PEARCE and E Schubert},
   doi = {10.1007/978-3-319-46282-0_23},
   editor = {R Kronland-Martinet and M Aramaki and S Ystad},
   isbn = {978-3-319-46281-3},
   journal = {Music, Mind, and Embodiment},
   pages = {358-370},
   publisher = {Springer},
   title = {A New Look at Musical Expectancy: The Veridical Versus the General in the Mental Organization of Music},
   year = {2016},
}
@article{QUINTON2016,
   author = {E QUINTON and M Sandler and S Dixon},
   issn = {1520-6149},
   journal = {IEEE International Conference on Acoustics Speech and Signal Processing},
   month = {4},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {ESTIMATION OF THE RELIABILITY OF MULTIPLE RHYTHM FEATURES EXTRACTION FROM A SINGLE DESCRIPTOR},
   year = {2016},
}
@article{REISS2016,
   abstract = {Over the last decade, there has been considerable debate over the benefits of recording and rendering high resolution audio beyond standard CD quality audio. This research involved a systematic review and meta-analysis (combining the results of numerous independent studies) to assess the ability of test subjects to perceive a difference between high resolution and standard (16 bit, 44.1 or 48 kHz) audio. Eighteen published experiments for which sufficient data could be obtained were included, providing a meta-analysis that combined over 400 participants in more than 12,500 trials. Results showed a small but statistically significant ability of test subjects to discriminate high resolution content, and this effect increased dramatically when test subjects received extensive training. This result was verified by a sensitivity analysis exploring different choices for the chosen studies and different analysis approaches. Potential biases in studies, effect of test methodology, experimental design, and choice of stimuli were also investigated. The overall conclusion is that the perceived fidelity of an audio recording and playback chain can be affected by operating beyond conventional resolution.},
   author = {J REISS},
   doi = {10.17743/jaes.2016.0015},
   issn = {1549-4950},
   issue = {6},
   journal = {Journal of the Audio Engineering Society},
   month = {6},
   pages = {364-379},
   publisher = {Audio Engineering Society},
   title = {A Meta-Analysis of High Resolution Audio Perceptual Evaluation},
   volume = {64},
   url = {tp://www.aes.org/},
   year = {2016},
}
@inproceedings{rodriguez2016,
   abstract = {Music content analysis (MCA) systems built using scattering transform features are reported quite successful in the GTZAN benchmark music dataset. In this paper, we seek to answer why. We first analyse the feature extraction and classification components of scattering-based MCA systems. This guides us to perform intervention experiments on three factors: train/test partition, classifier and recording spectrum. The partition intervention shows a decrease in the amount of reproduced ground truth by the resulting systems. We then replace the learning algorithm with a binary decision tree, and identify the impact of specific feature dimensions. We finally alter the spectral content related to such dimensions, which reveals that these scattering-based systems exploit acoustic information below 20 Hz to reproduce GTZAN ground truth. The source code to reproduce our experiments is available online.},
   author = {F RODRIGUEZ ALGARRA and B L Sturm and H Maruri-Aguilar},
   journal = {17th International Society for Music Information Retrieval Conference (ISMIR 2016)},
   month = {8},
   title = {Analysing Scattering-Based Music Content Analysis Systems: Where's the Music?},
   year = {2016},
}
@inproceedings{rodriguez2016b,
   author = {F J Rodriguez-Serrano and S Ewert and P Vera-Candeas and M Sandler},
   city = {Shanghai, China},
   journal = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
   title = {A Score-Informed Shift-Invariant Extension of Complex Matrix Factorization for Improving the Separation of Overlapped Partials in Music Recordings},
   year = {2016},
}
@article{Sigtia2016,
   author = {S Sigtia and E BENETOS and S Dixon},
   city = {Siddharth Sigtia, Queen Mary University of London, School of Electronic Engineering and Computer Science, Mile End Road, London, E1 4NS, United Kingdom},
   doi = {10.1109/TASLP.2016.2533858},
   issn = {2329-9290},
   issue = {5},
   journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
   month = {5},
   pages = {927-939},
   publisher = {IEEE},
   title = {An End-to-End Neural Network for Polyphonic Piano Music Transcription},
   volume = {24},
   url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7416164},
   year = {2016},
}
@article{Song2016,
   abstract = {Music both conveys and evokes emotions ,
and although both phenomena are widely studied, the
difference between them is often neglected. The purpose
of this study is to examine the difference between
perceived and induced emotion for Western popular
music using both categorical and dimensional models
of emotion, and to examine the influence of individual
listener differences on their emotion judgment. A total
of 80 musical excerpts were randomly selected from an
established dataset of 2,904 popular songs tagged with
one of the four words ‘‘happy,’’ ‘‘sad,’’ ‘‘angry,’’ or
‘‘relaxed’’ on the Last.FM web site. Participants listened
to the excerpts and rated perceived and induced emotion
on the categorical model and dimensional model, and
the reliability of emotion tags was evaluated according
to participants’ agreement with corresponding labels. In
addition, the Goldsmiths Musical Sophistication Index
(Gold-MSI) was used to assess participants’ musical
expertise and engagement. As expected, regardless of
the emotion model used, music evokes emotions similar
to the emotional quality perceived in music. Moreover,
emotion tags predict music emotion judgments. How-
ever, age, gender and three factors from Gold-MSI,
importance, emotion, and music training were found not
to predict listeners’ responses, nor the agreement with
tags.},
   author = {Y Song and S Dixon and M T Pearce and A R Halpern},
   doi = {10.1525/MP.2016.33.4.472},
   issue = {4},
   journal = {Music Perception},
   pages = {472-492},
   title = {Perceived and Induced Emotion Responses to Popular Music: Categorical and Dimensional Models},
   volume = {33},
   year = {2016},
}
@inproceedings{Stables2016,
   abstract = {© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.In music production, descriptive terminology is used to define perceived sound transformations. By understanding the underlying statistical features associated with these descriptions, we can aid the retrieval of contextually relevant processing parameters using natural language, and create intelligent systems capable of assisting in audio engineering. In this study, we present an analysis of a dataset containing descriptive terms gathered using a series of processing modules, embedded within a Digital Audio Workstation. By applying hierarchical clustering to the audio feature space, we show that similarity in term representations exists within and between transformation classes. Furthermore, the organisation of terms in low-dimensional timbre space can be explained using perceptual concepts such as size and dissonance. We conclude by performing Latent Semantic Indexing to show that similar groupings exist based on term frequency.},
   author = {R Stables and B De Man and S Enderby and J D Reiss and G Fazekas and T Wilmering},
   doi = {10.1145/2964284.2967238},
   isbn = {9781450336031},
   journal = {MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
   month = {10},
   pages = {337-341},
   title = {Semantic description of timbral transformations in music production},
   year = {2016},
}
@inproceedings{STOCKMAN2016,
   author = {A G STOCKMAN and D Al-Thani},
   journal = {The 5th International Workshop onn Collaboration: Human-Centered Issues & Interactivity Design, as part of The 2016 International Conference on Collaboration Technologies and Systems, Florida, USA, October-November 2016.},
   month = {10},
   title = {Reflections on the Research Methods Used in an Investigation of Cross-modal Collaborative Information Seeking},
   year = {2016},
}
@article{STOWELL2016,
   abstract = {Animals in groups often exchange calls, in patterns whose temporal structure may be influenced by contextual factors such as physical location and the social network structure of the group. We introduce a model-based analysis for temporal patterns of animal call timing, originally developed for networks of firing neurons. This has advantages over cross-correlation analysis in that it can correctly handle common-cause confounds and provides a generative model of call patterns with explicit parameters for the influences between individuals. It also has advantages over standard Markovian analysis in that it incorporates detailed temporal interactions which affect timing as well as sequencing of calls. Further, a fitted model can be used to generate novel synthetic call sequences. We apply the method to calls recorded from groups of domesticated zebra finch (Taeniopygia guttata) individuals. We find that the communication network in these groups has stable structure that persists from one day to the next, and that “kernels” reflecting the temporal range of influence have a characteristic structure for a calling individual’s effect on itself, its partner, and on others in the group. We further find characteristic patterns of influences by call type as well as by individual.},
   author = {D F STOWELL and L F Gill and D Clayton},
   doi = {10.1098/rsif.2016.0296},
   issn = {1742-5689},
   issue = {119},
   journal = {Journal of the Royal Society Interface},
   month = {6},
   publisher = {Royal Society, The},
   title = {Detailed temporal structure of communication networks in groups of songbirds},
   volume = {13},
   year = {2016},
}
@inproceedings{STOWELL2016b,
   abstract = {Many biological monitoring projects rely on acoustic detection of birds. Despite increasingly large datasets, this detection is often manual or semi-automatic, requiring manual tuning/postprocessing. We review the state of the art in automatic bird sound detection, and identify a widespread need for tuning-free and species-agnostic approaches. We introduce new datasets and an IEEE research challenge to address this need, to make possible the development of fully automatic algorithms for bird sound detection.},
   author = {D F STOWELL and M Wood and Y Stylianou and H Glotin},
   journal = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
   title = {Bird Detection In Audio: A Survey And A Challenge},
   year = {2016},
}
@inproceedings{Subramanian2016,
   author = {A Subramanian and N Cunha and D HERREMANS},
   journal = {XLVIII Simpósio Brasileiro de Pesquisa Operacional (SBPO)},
   month = {9},
   title = {Uma abordagem baseada em programação linear inteira para a geração de solos de guitarra},
   year = {2016},
}
@article{Sulyok2016,
   author = {C Sulyok and A MCPHERSON and C Harte},
   doi = {10.1007/s11047-016-9561-6},
   journal = {Natural Computing},
   month = {6},
   publisher = {Springer},
   title = {Evolving the process of a virtual composer},
   year = {2016},
}
@inproceedings{THALMANN2016,
   abstract = {This paper is about the Mobile Audio Ontology, a semantic audio framework for the design of novel music consumption experiences on mobile devices. The framework is based on the concept of the Dynamic Music Object which is an amalgamation of audio files, structural and analytical information extracted from the audio, and information about how it should be rendered in realtime. The Mobile Audio Ontology allows producers and distributors to specify a great variety of ways of playing back music in controlled indeterministic as well as adaptive and interactive ways. Users can map mobile sensor data, user interface controls, or autonomous control units hidden from the listener to any musical parameter exposed in the definition of a Dynamic Music Object. These mappings can also be made dependent on semantic and analytical information extracted from the audio.},
   author = {F S THALMANN and perez carillo and fazekas and wiggins and sandler},
   title = {The Mobile Audio Ontology: Experiencing Dynamic Music Objects on Mobile Devices},
   year = {2016},
}
@inproceedings{Theodorou2016,
   abstract = {How can performers detect and potentially respond to the reactions of a live audience? Audience members' physical movements provide one possible source of information about their engagement with a performance. Using a case study of the dance performance "Frames" that took place in Theatre Royal in Glasgow during March 2015, we examine patterns of audience movement during contemporary dance performances and explore how they relate to the dancer's movements. Video recordings of performers and audience were analysed using computer vision and data analysis techniques extracting facial expression, hand gestural and body movement data. We found that during the performance audiences move very little and have predominantly expressionless faces while hand gestures seem to play a significant role in the way audiences respond. This suggests that stillness i.e. the absence of motion may be an indicator of engagement.},
   author = {L Theodorou and P G T Healey and F Smeraldi},
   doi = {10.1145/2948910.2948928},
   isbn = {9781450343077},
   journal = {ACM International Conference Proceeding Series},
   month = {7},
   title = {Exploring audience behaviour during contemporary dance performances},
   volume = {05-06-July-2016},
   year = {2016},
}
@article{TIAN2016,
   author = {M TIAN and MARKB Sandler},
   issn = {2157-6912},
   journal = {ACM Transactions on Intelligent Systems and Technology},
   month = {11},
   title = {Towards Music Structural Segmentation Across Genres: Features, Structural Hypotheses and Annotation Principles},
   year = {2016},
}
@article{Turchet2016,
   author = {Luca Turchet and David Moffat and Ana Tajadura-Jiménez and Joshua D Reiss and Tony Stockman},
   journal = {Applied Acoustics},
   title = {What do your footsteps sound like? An investigation on interactive footstep sounds adjustment.},
   year = {2016},
}
@inproceedings{valero2016,
   abstract = {Note tracking constitutes a key process in Automatic Music Transcription as it derives a note-level transcription from a frame-based pitch activation representation. While this stage is commonly performed using a set of hand-crafted rules, this work presents an approach based on supervised classification which automatically infers these policies. An initial frame-level estimation provides the necessary information for segmenting each pitch band in single instances which are later classified as active or non-active note events. Preliminary results using classic classification strategies on a subset of the MAPS piano dataset report an improvement of up to a 15% when compared to the baseline considered for both frame-level and note-level assessment.},
   author = {J J Valero-Mas and E Benetos and J M Iñesta},
   journal = {https://sites.google.com/site/musicmachinelearning16/proceedings},
   month = {9},
   pages = {61-65},
   title = {Classification-based Note Tracking for Automatic Music Transcription},
   url = {https://sites.google.com/site/musicmachinelearning16/},
   year = {2016},
}
@article{valimaki2016,
   author = {V Välimäki and J Reiss},
   doi = {10.3390/app6050129},
   issn = {2076-3417},
   issue = {5},
   journal = {Applied Sciences},
   month = {5},
   pages = {129},
   publisher = {MDPI},
   title = {All About Audio Equalization: Solutions and Frontiers},
   volume = {6},
   year = {2016},
}
@article{Volk2016,
   author = {A Volk and E Chew and E H Margulis and C Anagnostopoulou},
   doi = {10.1080/09298215.2016.1232412},
   issn = {0929-8215},
   issue = {3},
   journal = {JOURNAL OF NEW MUSIC RESEARCH},
   month = {9},
   pages = {207-209},
   title = {Music Similarity: Concepts, Cognition and Computation},
   volume = {45},
   url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000385541200001&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a},
   year = {2016},
}
@article{Wang2016,
   abstract = {© 2014 IEEE.We propose a time difference of arrival (TDOA) estimation framework based on time-frequency inter-channel phase difference (IPD) to count and localize multiple acoustic sources in a reverberant environment using two distant microphones. The time-frequency (T-F) processing enables exploitation of the nonstationarity and sparsity of audio signals, increasing robustness to multiple sources and ambient noise. For inter-channel phase difference estimation, we use a cost function, which is equivalent to the generalized cross correlation with phase transform (GCC) algorithm and which is robust to spatial aliasing caused by large inter-microphone distances. To estimate the number of sources, we further propose an iterative contribution removal (ICR) algorithm to count and locate the sources using the peaks of the GCC function. In each iteration, we first use IPD to calculate the GCC function, whose highest peak is detected as the location of a sound source; then we detect the T-F bins that are associated with this source and remove them from the IPD set. The proposed ICR algorithm successfully solves the GCC peak ambiguities between multiple sources and multiple reverberant paths.},
   author = {L Wang and T K Hon and J D Reiss and A Cavallaro},
   doi = {10.1109/TASLP.2016.2533859},
   issn = {2329-9290},
   issue = {6},
   journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
   month = {6},
   pages = {1079-1093},
   title = {An Iterative Approach to Source Counting and Localization Using Two Distant Microphones},
   volume = {24},
   year = {2016},
}
@article{Wang2016b,
   abstract = {© 2015 IEEE.We investigate the problem of sensor and source joint localization using time-difference of arrivals (TDOAs) of an ad-hoc array. A major challenge is that the TDOAs contain unknown time offsets between asynchronous sensors. To address this problem, we propose a low-rank approximation method that does not need any prior knowledge of sensor and source locations or timing information. At first, we construct a pseudo time of arrival (TOA) matrix by introducing two sets of unknown timing parameters (source onset times and device capture times) into the current TDOA matrix. Then we propose a Gauss-Newton low-rank approximation algorithm to jointly identify the two sets of unknown timing parameters, exploiting the low-rank property embedded in the pseudo TOA matrix. We derive the boundaries of the timing parameters to reduce the initialization space and employ a multi-initialization scheme. Finally, we use the estimated timing parameters to correct the pseudo TOA matrix, which is further applied to sensor and source localization. Experimental results show that the proposed approach outperforms state-of-the-art algorithms.},
   author = {L Wang and T K Hon and J D Reiss and A Cavallaro},
   doi = {10.1109/TSP.2015.2498130},
   issn = {1053-587X},
   issue = {4},
   journal = {IEEE Transactions on Signal Processing},
   month = {2},
   pages = {1018-1033},
   title = {Self-localization of ad-hoc arrays using time difference of arrivals},
   volume = {64},
   year = {2016},
}
@article{Wang2016c,
   abstract = {© 2014 IEEE.The goal of music alignment is to map each temporal position in one version of a piece of music to the corresponding positions in other versions of the same piece. Despite considerable improvements in recent years, state-of-the-art methods still often fail to identify a correct alignment if versions differ substantially with respect to acoustic conditions or musical interpretation. To increase the robustness for these cases, we exploit in this work the availability of multiple versions of the piece to be aligned. By processing these jointly, we can supply the alignment process with additional examples of how a section might be interpreted or which acoustic conditions may arise. This way, we can use alignment information between two versions transitively to stabilize the alignment with a third version. Extending our previous work [1], we present two such joint alignment methods, progressive alignment and probabilistic profile, and discuss their fundamental differences and similarities on an algorithmic level. Our systematic experiments using 376 recordings of 9 pieces demonstrate that both methods can indeed improve the alignment accuracy and robustness over comparable pairwise methods. Further, we provide an in-depth analysis of the behavior of both joint alignment methods, studying the influence of parameters such as the number of performances available, comparing their computational costs, and investigating further strategies to increase both.},
   author = {S Wang and S Ewert and S Dixon},
   doi = {10.1109/TASLP.2016.2598318},
   issn = {2329-9290},
   issue = {11},
   journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
   month = {11},
   pages = {2132-2145},
   title = {Robust and efficient joint alignment of multiple musical performances},
   volume = {24},
   year = {2016},
}
@inproceedings{WILKINSON2016,
   abstract = {Spectral modelling represents an audio signal as
the sum of a finite number of partials – sinusoids tracked
through sequential analysis frames. With the goal of real-time
user-controllable synthesis in mind, we assume these observed
partials to be correlated functions of time, and that there exists
some lower-dimensional set of unobserved forcing functions
driving the partials through a set of differential equations.
Mapping of these unobserved functions to a user control space
provides us with a hybrid approach to synthesis in which
mechanistic controls are exposed to the user but the system’s
behavioural response to these mechanisms is learnt from data},
   author = {W J WILKINSON and D stowell and J reiss},
   journal = {DMRN+11: Digital Music Research Network Workshop Proceedings 2016},
   month = {12},
   publisher = {Centre for Digital Music, Queen Mary University of London},
   title = {Performable Spectral Synthesis via Low-Dimensional Modelling and
Control Mapping},
   url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/19345},
   year = {2016},
}
@inproceedings{Wilmering2016,
   abstract = {© Springer International Publishing AG 2016.This paper introduces the Audio Effect Ontology (AUFX-O) building on previous theoretical models describing audio processing units and workflows in the context of music production. We discuss important conceptualisations of different abstraction layers, their necessity to successfully model audio effects, and their application method. We present use cases concerning the use of effects in music production projects and the creation of audio effect metadata facilitating a linked data service exposing information about effect implementations. By doing so, we show how our model facilitates knowledge sharing, reproducibility and analysis of audio production workflows.},
   author = {T Wilmering and G Fazekas and M B Sandler},
   doi = {10.1007/978-3-319-46547-0_24},
   isbn = {9783319465463},
   issn = {0302-9743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   month = {1},
   pages = {229-237},
   title = {AUFX-O: Novel methods for the representation of audio processing workflows},
   volume = {9982 LNCS},
   year = {2016},
}
@inproceedings{YANG2016,
   author = {L YANG and SAYID-KHALID Rajab and E Chew},
   journal = {the 42nd International Computer Music Conference},
   month = {9},
   title = {AVA: A Graphical User Interface for Automatic Vibrato and Portamento Detection and Analysis},
   year = {2016},
}
@inproceedings{Zhang2016,
   abstract = {This paper presents a web-based application enabling au-
diences to collaboratively contribute to the creative pro-
cess during live music performances. The system aims at
enhancing audience engagement and creating new forms
of live music experiences. Interaction between audience
and performers is made possible through a client/server
architecture enabling bidirectional communication of cre-
ative data. Audience members can vote for pre-determined
musical attributes using a smartphone-friendly and cross-
platform web application. The system gathers audience
members' votes and provide feedback through visualisations
that can be tailored for speci c needs. In order to sup-
port multiple performers and large audiences, automatic
audience-to-performer groupings are handled by the appli-
cation. The framework was applied to support live interac-
tive musical improvisations where creative roles are shared
amongst audience and performers (Open Symphony). Qual-
itative analyses of user surveys highlighted very positive
feedback related to themes such as engagement and cre-
ativity and also identi ed further design challenges around
audience sense of control and latency.},
   author = {L Zhang and Y Wu and M BARTHET},
   journal = {International Conference on New Interfaces for Musical Expression},
   month = {7},
   title = {A Web Application for Audience Participation in Live Music Performance: The Open Symphony Use Case},
   year = {2016},
}
