@article{Abdallah2017,
   author = {S Abdallah and E Benetos and N Gold and S Hargreaves and T Weyde and D Wolff},
   doi = {10.1145/2983918},
   issn = {1556-4673},
   issue = {1},
   journal = {ACM Journal on Computing and Cultural Heritage},
   month = {1},
   publisher = {ACM},
   title = {The Digital Music Lab: A Big Data Infrastructure for Digital Musicology},
   volume = {10},
   url = {http://jocch.acm.org/},
   year = {2017},
}
@article{Agres2017,
   author = {K Agres and S Abdallah and M Pearce},
   doi = {10.1111/cogs.12477},
   issn = {0364-0213},
   journal = {Cognitive Science},
   month = {1},
   title = {Information-Theoretic Properties of Auditory Sequences Dynamically Influence Expectation and Memory},
   year = {2017},
}
@article{Agres2017b,
   abstract = {Â© 2017 Agres, Herremans, Bigo and Conklin.An empirical investigation of how local harmonic structures (e.g., chord progressions) contribute to the experience and enjoyment of uplifting trance (UT) music is presented. The connection between rhythmic and percussive elements and resulting trance-like states has been highlighted by musicologists, but no research, to our knowledge, has explored whether repeated harmonic elements influence affective responses in listeners of trance music. Two alternative hypotheses are discussed, the first highlighting the direct relationship between repetition/complexity and enjoyment, and the second based on the theoretical inverted-U relationship described by the Wundt curve. We investigate the connection between harmonic structure and subjective enjoyment through interdisciplinary behavioral and computational methods: First we discuss an experiment in which listeners provided enjoyment ratings for computer-generated UT anthems with varying levels of harmonic repetition and complexity. The anthems were generated using a statistical model trained on a corpus of 100 uplifting trance anthems created for this purpose, and harmonic structure was constrained by imposing particular repetition structures (semiotic patterns defining the order of chords in the sequence) on a professional UT music production template. Second, the relationship between harmonic structure and enjoyment is further explored using two computational approaches, one based on average Information Content, and another that measures average tonal tension between chords. The results of the listening experiment indicate that harmonic repetition does in fact contribute to the enjoyment of uplifting trance music. More compelling evidence was found for the second hypothesis discussed above, however some maximally repetitive structures were also preferred. Both computational models provide evidence for a Wundt-type relationship between complexity and enjoyment. By systematically manipulating the structure of chord progressions, we have discovered specific harmonic contexts in which repetitive or complex structure contribute to the enjoyment of uplifting trance music.},
   author = {K Agres and D Herremans and L Bigo and D Conklin},
   doi = {10.3389/fpsyg.2016.01999},
   issue = {JAN},
   journal = {Frontiers in Psychology},
   month = {1},
   title = {Harmonic Structure Predicts the Enjoyment of Uplifting Trance Music},
   volume = {7},
   year = {2017},
}
@inproceedings{Armitage2017,
   abstract = {Many digital musical instrument design frameworks have been proposed that are well suited for analysis and comparison. However, not all provide applicable design suggestions, especially where subtle, important details are concerned. Using traditional lutherie as a model, we conducted a series of interviews to explore how violin makers “go beyond the obvious”, and how players perceive and describe subtle details of instrumental quality. We find that lutherie frameworks provide clear design methods, but are not enough to make a fine violin. Success comes after acquiring sufficient tacit knowledge, which enables detailed craft through subjective, empirical methods. Testing instruments for subtle qualities was suggested to be a different skill to playing. Whilst players are able to identify some specific details about instrumental quality by comparison, these are often not actionable, and important aspects of “sound and feeling” are much more difficult to describe. In the DMI domain, we introduce the term NIMEcraft to describe subtle differences between otherwise identical instruments and their underlying design processes, and consider how to improve the dissemination of NIMEcraft.},
   author = {J Armitage and F Morreale and A McPherson},
   journal = {http://www.nime.org/archives/},
   month = {5},
   publisher = {New Instruments for Musical Expression},
   title = {"The finer the musician, the smaller the details": NIMEcraft under the microscope},
   url = {http://www.instrumentslab.org/},
   year = {2017},
}
@inproceedings{Bechhofer2017,
   abstract = {We describe the publication of a linked data set exposing metadata from the Internet Archive Live Music Archive along with detailed feature analysis data of the audio files contained in the archive. The collection is linked to existing musical and geographical resources allowing for the extraction of useful or nteresting subsets of data using additional metadata. The collection is published using a ‘layered’ approach, aggregating the original information with links and specialised analyses, and forms a valuable resource for those investigating or developing audio analysis tools and workflows.},
   author = {S Bechhofer and K Page and D Weigl and G FAZEKAS and T Wilmering},
   doi = {10.1007/978-3-319-68204-4_3},
   journal = {The Semantic Web, proc. of the 16th International Semantic Web Conference (ISWC), Oct. 21-25, Vienna, Austria},
   month = {10},
   note = {date-added: 2017-12-22 15:39:21 +0000
date-modified: 2017-12-22 15:53:18 +0000
keywords: Linked Data, Semantic Audio, Semantic Web, live music archive
local-url: https://link.springer.com/chapter/10.1007/978-3-319-68204-4_3
bdsk-url-1: https://iswc2017.semanticweb.org/wp-content/uploads/papers/MainProceedings/221.pdf
bdsk-url-2: https://dx.doi.org/10.1007/978-3-319-68204-4_3},
   publisher = {Springer, Cham},
   title = {Linked Data Publication of Live Music Archives and Analyses},
   volume = {10588},
   url = {https://iswc2017.semanticweb.org/wp-content/uploads/papers/MainProceedings/221.pdf},
   year = {2017},
}
@inproceedings{Benetos2017,
   abstract = {In this paper, a system for automatic transcription of multiple-instrument polyphonic music is proposed, which supports tracking multiple concurrent notes using linear dynamical systems (LDS). The system is based on a spectrogram factorisation model which extends probabilistic latent component analysis (PLCA), and supports the detection of multiple pitches, instrument contributions, and pitch deviations. In order to jointly track multiple concurrent pitches, the use of LDS as prior to the PLCA model is proposed. LDS parameters are learned in a training stage using score-informed transcriptions; for LDS inference, online and offline variants are evaluated. The MAPS piano music dataset and the Bach10 multi-instrument dataset are used for note tracking experiments, with the latter dataset also being evaluated with respect to instrument assignment performance. Results show that the proposed LDS-based method can successfully track multiple concurrent notes, leading to an improvement of over 3% in terms of note-based F-measure for both datasets over benchmark note tracking approaches.},
   author = {E Benetos},
   journal = {2017 AES International Conference on Semantic Audio},
   month = {6},
   publisher = {Audio Engineering Society},
   title = {Polyphonic note and instrument tracking using linear dynamical systems},
   url = {http://www.aes.org/conferences/2017/semantic/},
   year = {2017},
}
@article{Benetos2017b,
   abstract = {In this paper, a system for polyphonic sound event detection and tracking is proposed, based on spectrogram factorisation techniques and state space models. The system extends probabilistic latent component analysis (PLCA) and is modelled around a 4-dimensional spectral template dictionary of frequency, sound event class, exemplar index, and sound state. In order to jointly track multiple overlapping sound events over time, the integration of linear dynamical systems (LDS) within the PLCA inference is proposed. The system assumes that the PLCA sound event activation is the (noisy) observation in an LDS, with the latent states corresponding to the true event activations. LDS training is achieved using fully observed data, making use of ground truth-informed event activations produced by the PLCA-based model. Several LDS variants are evaluated, using polyphonic datasets of office sounds generated from an acoustic scene simulator, as well as real and synthesized monophonic datasets for comparative purposes. Results show that the integration of LDS tracking within PLCA leads to an improvement of +8.5-10.5% in terms of frame-based F-measure as compared to the use of the PLCA model alone. In addition, the proposed system outperforms several state-of-the-art methods for the task of polyphonic sound event detection.},
   author = {E Benetos and G Lafay and M Lagrange and M D Plumbley},
   doi = {10.1109/TASLP.2017.2690576},
   issn = {2329-9304},
   issue = {6},
   journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
   month = {5},
   pages = {1266-1277},
   publisher = {IEEE},
   title = {Polyphonic Sound Event Tracking using Linear Dynamical Systems},
   volume = {25},
   url = {http://ieeexplore.ieee.org/document/7933041/},
   year = {2017},
}
@article{Bengler2017,
   author = {B Bengler and F Martin and N Bryan-Kinns and C Frauenberger and J Makhaeva and K Spiel and R Vishkaie and L Jones},
   doi = {10.1145/3162013},
   issn = {1072-5520},
   issue = {1},
   journal = {Interactions},
   month = {12},
   pages = {8-11},
   title = {Demo hour},
   volume = {25},
   year = {2017},
}
@inproceedings{BIN2017,
   abstract = {This paper presents a methodology for the study of audience perception of live performances, using a combined approach of post-hoc and real-time data. We conducted a study that queried audience enjoyment and their perception of error in digital musical instrument (DMI) performance. We collected quantitative and qualitative data from the participants (N=64) via paper survey after each performance and at the end of the concert, and during the performances spectators were invited to indicate moments of enjoyment and incidences of error using a two-button mobile app interface. We demonstrate that real-time indication of error does not translate to reported non-enjoyment and post-hoc and real-time data sets are not necessarily consistent for each participant. In conclusion we make the case for a combined approach to audience studies in live performance contexts.},
   author = {S M A BIN and F MORREALE and N BRYAN-KINNS and A MCPHERSON},
   month = {9},
   title = {In-the-moment and beyond: Combining post-hoc and real time data for the study of audience perception of electronic music performance},
   year = {2017},
}
@inproceedings{Bin2017b,
   abstract = {This paper explores the relative effect of gesture size on audience perception of digital musical instrument (DMI) performance. In a study involving a total audience of 28 people (split into 2 groups of 13 and 15), we used a small and large version of a DMI to examine how the size of performers' gestures might differ, and how this affects post-hoc audience ratings of enjoyment, interest and understanding, as well as their indications of `enjoyment' and `error' in real time. For each audience we held two 5-minute performances, the first on a custom-designed percussion DMI, and the second on a laptop. The DMI used in each performance was made up of three elements identical in shape, materiality, interaction and sound, but the physical size was different: For one each element was approx 12x10x5cm, and the other was about 3.5 times bigger (approx. 40x30x20cm). Data was collected both during and after the performance via post-hoc and real-time methods. We found that beyond a performance simply involving physical gesture, the size of gesture has an impact on audience ratings. In this paper we detail this study and its results, and present the implications that this finding has for DMI design.},
   author = {S M A Bin and N Bryan-Kinns and A McPherson},
   month = {10},
   title = {Hands where we can see them! Investigating the impact of gesture size on audience perception},
   year = {2017},
}
@article{Cameron2017,
   abstract = {Rhythm is an essential part of the structure, behaviour, and aesthetics of music. However, the cog- nitive processing that underlies the perception of musical rhythm is not fully understood. In this study, we tested whether rhythm perception is influenced by three factors: musical training, the presence of expressive performance cues in human-performed music, and the broader musical con- text. We compared musicians and nonmusicians’ similarity ratings for pairs of rhythms taken from Steve Reich’s Clapping Music. The rhythms were heard both in isolation and in musical context and both with and without expressive performance cues. The results revealed that rhythm perception is influenced by the experimental conditions: rhythms heard in musical context were rated as less similar than those heard in isolation; musicians’ ratings were unaffected by expressive performance, but nonmusicians rated expressively performed rhythms as less similar than those with exact timing; and expressively-performed rhythms were rated as less similar compared to rhythms with exact timing when heard in isolation but not when heard in musical context. The results also showed asymmetrical perception: the order in which two rhythms were heard influenced their perceived similarity. Analyses suggest that this asymmetry was driven by the internal coherence of rhythms, as measured by normalized Pairwise Variability Index (nPVI). As predicted, rhythms were perceived as less similar when the first rhythm in a pair had greater coherence (lower nPVI) than the second rhythm, compared to when the rhythms were heard in the opposite order.},
   author = {D Cameron and K Potter and G Wiggins and M T PEARCE},
   doi = {10.1163/22134468-00002085},
   journal = {Timing and Time Perception},
   month = {6},
   title = {Perception of Rhythmic Similarity is Asymmetrical, and Is Influenced by Musical Training, Expressive Performance, and Musical Context},
   year = {2017},
}
@inproceedings{CHOI2017,
   author = {K CHOI and G FAZEKAS and M SANDLER and C Kyunghyun},
   month = {10},
   title = {Transfer learning for music classification and regression tasks},
   year = {2017},
}
@article{Chourdakis2017,
   abstract = {We propose a design of an adaptive digital audio effect for artificial reverberation, controlled
directly by desired reverberation characteristics, that allows it to learn from the user in a super-
vised way. The user provides monophonic examples of desired reverberation characteristics
for individual tracks taken from the Open Multitrack Testbed. We use this data to train a set of
models to automatically apply reverberation to similar tracks. We evaluate those models using
classifier f1-scores, mean squared errors, and multi-stimulus listening tests.},
   author = {E Chourdakis and J REISS},
   issn = {1549-4950},
   journal = {Journal of the Audio Engineering Society},
   month = {2},
   publisher = {Audio Engineering Society},
   title = {A Machine-Learning Approach to Application of Intelligent Artificial Reverberation},
   volume = {65},
   year = {2017},
}
@inproceedings{CHOURDAKIS2017b,
   abstract = {This paper proposes a method for learning
how to generate narrative by recombining sentences
from a previous collection. Given a corpus
of story events categorised into 9 topics,
we approximate a deep reinforcement learning
agent policy to recombine them in order
to satisfy narrative structure. We also propose
an evaluation of such a system. The evaluation
is based on coherence, interest, and topic,
in order to figure how much sense the generated
stories make, how interesting they are,
and examine whether new narrative topics can
emerge.},
   author = {E T CHOURDAKIS and J D REISS},
   journal = {http://aclanthology.info/events/ws-2017},
   month = {9},
   title = {Constructing narrative using a generative model and continuous action policies},
   year = {2017},
}
@book_section{Deacon2017,
   abstract = {The Objects VR interface and study explores interactive music and virtual reality, focusing on user experience, understanding of musical functionality, and interaction issues. Our system offers spatio-temporal music interaction using 3D geometric shapes and their designed relationships. Control is provided by tracking of the hands, and the experience is rendered across a head-mounted display with binaural sound presented over headphones. The evaluation of the system uses a mixed methods approach based on semi-structured interviews, surveys and video-based interaction analysis. On average the system was positively received in terms of interview self-report, metrics for spatial presence and creative support. Interaction analysis and interview thematic analysis also revealed instances of frustration with interaction and levels of confusion with system functionality. Our results allow reflection on design criteria and discussion of implications for facilitating music engagement in virtual reality. Finally our work discusses the effectiveness of measures with respect to future evaluation of novel interactive music systems in virtual reality.},
   author = {T Deacon and T Stockman and M Barthet},
   doi = {10.1007/978-3-319-67738-5_12},
   isbn = {9783319677378},
   issn = {0302-9743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   month = {9},
   pages = {192-216},
   publisher = {Springer},
   title = {User experience in an interactive music virtual reality system: An exploratory study},
   volume = {10525 LNCS},
   year = {2017},
}
@article{DiGiorgi2017,
   author = {B Di Giorgi and S Dixon and M Zanoni and A Sarti},
   doi = {10.1109/TASLP.2017.2756443},
   issn = {2329-9290},
   journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
   month = {9},
   pages = {1},
   title = {A data-driven model of tonal chord sequence complexity},
   year = {2017},
}
@inproceedings{Ewert2017,
   abstract = {© 2017 IEEE. Many success stories involving deep neural networks are instances of supervised learning, where available labels power gradient-based learning methods. Creating such labels, however, can be expensive and thus there is increasing interest in weak labels which only provide coarse information, with uncertainty regarding time, location or value. Using such labels often leads to considerable challenges for the learning process. Current methods for weak-label training often employ standard supervised approaches that additionally reassign or prune labels during the learning process. The information gain, however, is often limited as only the importance of labels where the network already yields reasonable results is boosted. We propose treating weak-label training as an unsupervised problem and use the labels to guide the representation learning to induce structure. To this end, we propose two autoencoder extensions: class activity penalties and structured dropout. We demonstrate the capabilities of our approach in the context of score-informed source separation of music.},
   author = {S Ewert and M B Sandler},
   doi = {10.1109/ICASSP.2017.7952562},
   isbn = {9781509041176},
   issn = {1520-6149},
   journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
   month = {6},
   pages = {2277-2281},
   title = {Structured dropout for weak label and multi-instance learning and its application to score-informed source separation},
   year = {2017},
}
@inproceedings{Ewert2017b,
   abstract = {© 2017 IEEE. A central goal in automatic music transcription is to detect individual note events in music recordings. An important variant is instrument-dependent music transcription where methods can use calibration data for the instruments in use. However, despite the additional information, results rarely exceed an f-measure of 80%. As a potential explanation, the transcription problem can be shown to be badly conditioned and thus relies on appropriate regularization. A recently proposed method employs a mixture of simple, convex regularizers (to stabilize the parameter estimation process) and more complex terms (to encourage more meaningful structure). In this paper, we present two extensions to this method. First, we integrate a computational loudness model to better differentiate real from spurious note detections. Second, we employ (Bidirectional) Long Short Term Memory networks to re-weight the likelihood of detected note constellations. Despite their simplicity, our two extensions lead to a drop of about 35% in note error rate compared to the state-of-the-art.},
   author = {S Ewert and M B Sandler},
   doi = {10.1109/WASPAA.2017.8170012},
   isbn = {9781538616321},
   journal = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
   month = {12},
   pages = {146-150},
   title = {An augmented lagrangian method for piano transcription using equal loudness thresholding and lstm-based decoding},
   volume = {2017-October},
   year = {2017},
}
@inproceedings{Fano2017,
   abstract = {In live and studio recordings unexpected sound events often lead to interferences in the signal. For non-stationary interferences, sound source separation techniques can be used to reduce the interference level in the recording. In this context, we present a novel approach combining the strengths of two algorithmic families: NMF and KAM. The recent KAM approach applies robust statistics on frames selected by a source-specific kernel to perform source separation. Based on semi-supervised NMF, we extend this approach in two ways. First, we locate the interference in the recording based on detected NMF activity. Second, we improve the kernel-based frame selection by incorporating an NMF-based estimate of the clean music signal. Further, we introduce a temporal context in the kernel, taking some musical structure into account. Our experiments show improved separation quality for our proposed method over a state-of-the-art approach for interference reduction.},
   author = {D FANO YELA and S E Ewert and M S Sandler and D F FitzGerald},
   title = {INTERFERENCE REDUCTION IN MUSIC RECORDINGS COMBINING KERNEL ADDITIVE MODELLING AND NON-NEGATIVE MATRIX FACTORIZATION},
   year = {2017},
}
@article{Halpern2017,
   abstract = {Part of musical understanding and enjoyment stems from the ability to accurately predict what note (or one of a small set of notes) is likely to follow after hearing the first part of a melody. Selective violation of expectations can add to aesthetic response but radical or frequent violations are likely to be disliked or not comprehended. In this study we investigated whether a lifetime of exposure to music among untrained older adults would enhance their reaction to unexpected endings of unfamiliar melodies. Older and younger adults listened to melodies that had expected or unexpected ending notes, according to Western music theory. Ratings of goodness-of-fit were similar in the groups, as was ERP response to the note onset (N1). However, in later time windows (P200 and Late Positive Component), the amplitude of a response to unexpected and expected endings was both larger in older adults, corresponding to greater sensitivity, and more widespread in locus, consistent with a dedifferentiation pattern. Lateralization patterns also differed. We conclude that older adults refine their understanding of this important aspect of music throughout life, with the ability supported by changing patterns of neural activity.},
   author = {A R Halpern and I Zioga and M Shankleman and J Lindsen and M T Pearce and J Bhattacharya},
   doi = {10.1016/j.bandc.2016.12.006},
   journal = {Brain Cogn},
   month = {4},
   pages = {1-9},
   title = {That note sounds wrong! Age-related effects in processing of musical expectation.},
   volume = {113},
   url = {https://www.ncbi.nlm.nih.gov/pubmed/28064077},
   year = {2017},
}
@article{HARRISON2017,
   abstract = {This paper presents a prototype system for adapting the bass guitar for one-handed musicians. We discuss existing solutions to accessible musical instruments, followed by the results of an online survey of bass guitarists, which informed the design of a prototype bass guitar adaptation. The adaptation comprises a foot-operated MIDI controller with a solenoid-actuated fretting mechanism, providing access to six frets across two strings of the bass. A study involving six bassists rehearsing and writing a bass guitar accompaniment with the adapted bass highlighted unexpected facets of bass guitar playing, and provided insights into the design of future accessible string instruments.},
   author = {J T F HARRISON and A P McPherson},
   doi = {10.1080/09298215.2017.1340485},
   issn = {0929-8215},
   journal = {Journal of New Music Research},
   month = {6},
   publisher = {Taylor & Francis (Routledge)},
   title = {Adapting the Bass Guitar for One-Handed Playing},
   year = {2017},
}
@article{HEALEY2017,
   author = {P G T HEALEY and Duffy},
   doi = {10.1386/jmte.10.1.5_1},
   issn = {1752-7066},
   issue = {1},
   journal = {Journal of Music, Technology and Education},
   publisher = {Intellect},
   title = {A New Medium for Remote Music Tuition},
   volume = {10},
   year = {2017},
}
@inproceedings{HERREMANS2017,
   author = {D HERREMANS and C H Chuan},
   journal = {IEEEInternational Conference on Semantic Computing},
   month = {1},
   title = {A multi-modal platform for semantic music analysis: visualizing audio- and score-based tension},
   year = {2017},
}
@inproceedings{Herremans2017b,
   abstract = {© 2017 Copyright held by the owner/author(s). Emotional response to music is often represented on a two-dimensional arousal-valence space without reference to score information that may provide critical cues to explain the observed data. To bridge this gap, we present IMMA-Emo, an integrated software system for visualising emotion data aligned with music audio and score, so as to provide an intuitive way to interactively visualise and analyse music emotion data. The visual interface also allows for the comparison of multiple emotion time series. The IMMA-Emo system builds on the online interactive Multi-modal Music Analysis (IMMA) system. Two examples demonstrating the capabilities of the IMMA-Emo system are drawn from an experiment set up to collect arousal-valence ratings based on participants' perceived emotions during a live performance. Direct observation of corresponding score parts and aural input from the recording allow explanatory factors to be identified for the ratings and changes in the ratings.},
   author = {D Herremans and S Yang and C H Chuan and M Barthet and E Chew},
   doi = {10.1145/3123514.3123545},
   isbn = {9781450353731},
   journal = {ACM International Conference Proceeding Series},
   month = {8},
   title = {IMMA-Emo: A multimodal interface for visualising score- and audio-synchronised emotion annotations},
   volume = {Part F131930},
   year = {2017},
}
@article{Herremans2017c,
   author = {D Herremans and C-H Chuan and E Chew},
   doi = {10.1145/3108242},
   issn = {0360-0300},
   issue = {5},
   journal = {ACM COMPUTING SURVEYS},
   month = {11},
   title = {A Functional Taxonomy of Music Generation Systems},
   volume = {50},
   url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000418294100008&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a},
   year = {2017},
}
@inproceedings{Jack2017,
   abstract = {Expression in musical practice is inextricably tied to the touch of the performer. In digital musical instruments (DMIs) the relationship of touch to sound is indirect: The nuances and fine detail of performer control can be flattened and limited during the translation of physical gesture to physical sound. The locus of this research is in the contact made between performer and DMI: focusing on this area can grant insight on fundamental issues of human computer interaction, particularly regarding intimate and expressive control of tangible interfaces. In this paper I present my research on this topic so far, which includes empirical studies that focus on specific parameters of performance where touch plays an integral role. The first study investigates how dynamic vibrations in an instrument's body can guide the hand of a performer and assist with intonation. The second study looks at asynchrony between action and sound and the influence this latency has on the perceived quality of an instrument.},
   author = {R H Jack and T Stockman and A McPherson},
   doi = {10.1145/3024969.3025042},
   isbn = {9781450346764},
   journal = {TEI 2017 - Proceedings of the 11th International Conference on Tangible, Embedded, and Embodied Interaction},
   month = {3},
   pages = {717-720},
   title = {Maintaining and constraining performer touch in the design of digital musical instruments},
   year = {2017},
}
@inproceedings{Jack2017b,
   abstract = {© 2017 Copyright held by the owner/author(s). This paper presents an observational study of the interaction of professional percussionists with a simplified hand percussion instrument. We reflect on how the sound-producing gestural language of the percussionists developed over the course of an hour session, focusing on the elements of their gestural vocabulary that remained in place at the end of the session, and on those that ceased to be used. From these observations we propose a model of movement based digital musical instruments as a projection downwards from a multidimensional body language to a reduced set of sonic features or behaviours. Many factors of an instrument's design, above and beyond the mapping of sensor degrees of freedom to dimensions of control, condition the way this projection downwards happens. We argue that there exists a world of richness of gesture beyond that which the sensors capture, but which can be implicitly captured by the design of the instrument through its physicality, constituent materials and form. We provide a case study of this model in action.},
   author = {R H Jack and T Stockman and A McPherson},
   doi = {10.1145/3077981.3078039},
   isbn = {1595930361},
   journal = {ACM International Conference Proceeding Series},
   month = {6},
   title = {Rich gesture, reduced control: The influence of constrained mappings on performance technique},
   volume = {Part F129150},
   year = {2017},
}
@inproceedings{Jillings2017,
   abstract = {© 2017 IEEE. Zero latency convolution typically uses the Direct Form approach, requiring a large amount of computational resources for every additional sample in the impulse response. A number of methods have been developed to reduce the computational cost of very large signal convolution. However these all introduce latency into the system. In some scenarios this is not acceptable and must be removed. Modern computer systems hold multiple processor architectures, with their own strengths and weaknesses for the purpose of convolution. This paper shows how correctly combining these processors can lead to a powerful system which can be deployed for real-Time, zero-latency large signal convolution.},
   author = {N Jillings and J D Reiss and R Stables},
   doi = {10.1109/WASPAA.2017.8170051},
   isbn = {9781538616321},
   journal = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
   month = {12},
   pages = {339-343},
   title = {Zero-Delay large signal convolution using multiple processor architectures},
   volume = {2017-October},
   year = {2017},
}
@inproceedings{KUDUMAKIS2017,
   author = {P KUDUMAKIS and M SANDLER},
   doi = {10.26494/DMRN.2017.30583},
   editor = {P KUDUMAKIS and M SANDLER},
   month = {12},
   title = {DMRN+12: Digital Music Research Network Workshop Proceedings 2017},
   year = {2017},
}
@inproceedings{Lafay2017,
   abstract = {As part of the 2016 public evaluation challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2016), the second task focused on evaluating sound event detection systems using synthetic mixtures of office sounds. This task, which follows the 'Event Detection - Office Synthetic' task of DCASE 2013, studies the behaviour of tested algorithms when facing controlled levels of audio complexity with respect to background noise and polyphony/density, with the added benefit of a very accurate ground truth. This paper presents the task formulation, evaluation metrics, submitted systems, and provides a statistical analysis of the results achieved, with respect to various aspects of the evaluation dataset.},
   author = {G Lafay and E Benetos and M Lagrange},
   doi = {10.1109/WASPAA.2017.8169985},
   journal = {http://www.waspaa.com/},
   month = {10},
   pages = {11-15},
   publisher = {IEEE},
   title = {Sound Event Detection in Synthetic Audio: Analysis of the DCASE 2016 Task Results},
   url = {http://www.waspaa.com/},
   year = {2017},
}
@inproceedings{Liang2017,
   abstract = {Automatic detection of piano pedalling techniques is challenging as it is comprised of subtle nuances of piano timbres. In this paper, we address this problem on single notes using decision-tree-based support vector machines. Features are extracted from harmonics and residuals based on physical acoustics considerations and signal observations. We consider four distinct pedalling techniques on the sustain pedal (anticipatory full, anticipatory half, legato full and legato half pedalling) and create a new isolated-note dataset consisting of different pitches and velocities for each pedalling technique plus notes played without pedal. Our results using cross-validation trails show the effectiveness of the designed features and the trained classifiers for discriminating pedalling techniques.},
   author = {B Liang and G Fazekas and M Sandler},
   journal = {http://www.aes.org/e-lib/},
   month = {10},
   publisher = {Audio Engineering Society},
   title = {Detection of Piano Pedaling Techniques on the Sustain Pedal},
   url = {http://www.aes.org/e-lib/browse.cfm?elib=19209},
   year = {2017},
}
@inproceedings{Liang2017b,
   abstract = {This paper presents the results of a study of piano pedalling techniques on the sustain pedal using a newly designed measurement system named Piano Pedaller. The system is comprised of an optical sensor mounted in the piano pedal bearing block and an embedded platform for recording audio and sensor data. This enables recording the pedalling gesture of real players and the piano sound under normal playing conditions. Using the gesture data collected from the system, the task of classifying these data by pedalling technique was undertaken using a Support Vector Machine (SVM). Results can be visualised in an audio based score following application to show pedalling together with the player's position in the score.},
   author = {B Liang and G Fazekas and A McPherson and M Sandler},
   journal = {http://www.nime.org/archives/},
   month = {5},
   publisher = {New Instruments for Musical Expression},
   title = {Piano Pedaller: A Measurement System for Classification and Visualisation of Piano Pedalling Techniques},
   url = {http://homes.create.aau.dk/dano/nime17/papers/0062/index.html},
   year = {2017},
}
@inproceedings{Marengo2017,
   abstract = {As many cultural institutions are publishing digital heritage material on the web, a new type of user emerged, that casually interacts with the art collection in his/her free time, driven by intrinsic curiosity more than by a professional duty or an informational goal. Can choices in how the interaction with data is structured increase engagement of such users? In our exploratory study, we use the WikiArt project as a case study to analyse how users approach search interfaces for free exploration. Our preliminary results show that, despite the remarkable diversity of artworks available, users rely on familiarity as their main criterion to navigate the website; they stay within known topics and rarely discover new ones. Users show interest in heterogeneous datasets, but their engagement is rarely sustained, while the presence of slightly unrelated artworks in a set can increase curiosity and self-reflection. Finally, we discuss the role of the database’s perceived size on users’ expectations.},
   author = {L Marengo and G FAZEKAS and A TOMBROS},
   doi = {10.1007/978-3-319-58753-0_82},
   journal = {19th International Conference on Human-Computer Interaction (HCI’17), 9-14 July, Vancouver, Canada},
   month = {7},
   note = {date-added: 2017-12-22 18:06:25 +0000
date-modified: 2017-12-22 18:42:08 +0000
keywords: information retrieval, information seeking, casual interaction, curiosity, engagement
publisher-url: https://link.springer.com/chapter/10.1007%2F978-3-319-58753-0_82
bdsk-url-1: http://www.semanticaudio.net/files/papers/marengo2017hci.pdf
bdsk-url-2: https://dx.doi.org/10.1007/978-3-319-58753-0_82},
   publisher = {Springer, Cham},
   title = {The Interaction of Casual Users with Digital Collections of Visual Art: An Exploratory Study of the WikiArt Website},
   volume = {714},
   year = {2017},
}
@inproceedings{McArthur2017,
   abstract = {© 2017 Copyright is held by the owner/author(s). Spatial audio is enjoying a surge in attention in both scene and object based paradigms, due to the trend for, and accessibility of, immersive experience. This has been enabled through convergence in computing enhancements, component size reduction, and associated price reductions. For the first time, applications such as virtual reality (VR) are technologies for the consumer. Audio for VR is captured to provide a counterpart to the video or animated image, and can be rendered to combine elements of physical and psychoacoustic modelling, as well as artistic design. Given that distance is an inherent property of spatial audio, that it can augment sound's efficacy in cueing user attention (a problem which practitioners are seeking to solve), and that conventional film sound practices have intentionally exploited its use, the absence of research on its implementation and effects in immersive environments is notable. This paper sets out the case for its importance, from a perspective of research and practice. It focuses on cinematic VR, whose challenges for spatialized audio are clear, and at times stretches beyond the restrictions specific to distance in audio for VR, into more general audio constraints.},
   author = {A McArthur and M Sandler and R Stewart},
   doi = {10.1145/3123514.3123530},
   isbn = {9781450353731},
   journal = {ACM International Conference Proceeding Series},
   month = {8},
   title = {Distance in audio for VR: Constraints and opportunities},
   volume = {Part F131930},
   year = {2017},
}
@article{McLeod2017,
   abstract = {This paper presents a method for automatic music transcription applied to audio recordings of a cappella performances with multiple singers. We propose a system for multi-pitch detection and voice assignment that integrates an acoustic and a music language model. The acoustic model performs spectrogram decomposition, extending probabilistic latent component analysis (PLCA) using a six-dimensional dictionary with pre-extracted log-spectral templates. The music language model performs voice separation and assignment using hidden Markov models that apply musicological assumptions. By integrating the two models, the system is able to detect multiple concurrent pitches in polyphonic vocal music and assign each detected pitch to a specific voice type such as soprano, alto, tenor or bass (SATB). We compare our system against multiple baselines, achieving state-of-the-art results for both multi-pitch detection and voice assignment on a dataset of Bach chorales and another of barbershop quartets. We also present an additional evaluation of our system using varied pitch tolerance levels to investigate its performance at 20-cent pitch resolution.},
   author = {A McLeod and R Schramm and M Steedman and E BENETOS},
   doi = {10.3390/app7121285},
   issn = {2076-3417},
   issue = {12},
   journal = {Applied Sciences},
   month = {12},
   publisher = {MDPI AG},
   title = {Automatic Transcription of Polyphonic Vocal Music},
   volume = {7},
   url = {http://www.mdpi.com/2076-3417/7/12/1285},
   year = {2017},
}
@article{MEHRABI2017,
   abstract = {Vocal imitations are often used to convey sonic ideas [Lemaitre, Dessein, Susini, and Aura.
(2011). Ecol. Psych.
23
(4), 267–307]. For computer based systems to interpret these vocalisations,
it is advantageous to apply knowledge of what happens when people vocalise sounds where the
acoustic features have different temporal envelopes. In the present study, 19 experienced musi-
cians and music producers were asked to imitate 44 sounds with one or two feature envelopes
applied. The study addresses two main questions: (1) How accurately can people imitate ramp and
modulation envelopes for pitch, loudness, and spectral centroid?; (2) What happens to this accu-
racy when people are asked to imitate two feature envelopes simultaneously? The results show
that experienced musicians can imitate pitch, loudness, and spectral centroid accurately, and that
imitation accuracy is generally preserved when the imitated stimuli combine two, non-necessarily
congruent features. This demonstrates the viability of using the voice as a natural means of
expressing time series of two features simultaneously.},
   author = {A MEHRABI and S Dixon and Sandler},
   doi = {10.1121/1.4974825},
   issn = {1520-8524},
   issue = {2},
   journal = {Journal of the Acoustical Society of America},
   month = {2},
   pages = {783-796},
   publisher = {Acoustical Society of America},
   title = {Vocal imitation of synthesised sounds varying in pitch, loudness and spectral centroid},
   volume = {141},
   url = {http://asa.scitation.org/doi/abs/10.1121/1.4974825},
   year = {2017},
}
@article{Mehrabi2017b,
   abstract = {© 2017. When radio podcasts are produced from previously broadcast material, 30-second "thumbnails" of songs that featured in the original program are often included. Such thumbnails are made up of continuous or concatenated sections from a song and provide the audience with a summary of the music content. However, editing full-length songs down to representative thumbnails is a labor intensive process, particularly when concatenating multiple song sections. This presents an ideal application for automatic music editing tools and raises the question of how a piece of music is best summarized for this task. To gain insight into this problem we asked 120 listeners to rate the quality of thumbnails generated by eight methods (five automatic and three manual). When asked to judge overall editing quality (on a five point Likert scale) listeners gave higher ratings to methods where the edit points were quantized to bar positions, although we found no preference for structural content such as the chorus. Ratings for two automatic editing methods (one containing the chorus, one containing only the intro and outro) were not significantly different to their manual counterparts. This result suggests that the automatic editing methods applied here can be used to create production quality thumbnails.},
   author = {A Mehrabi and C Harte and C Baume and S Dixon},
   doi = {10.17743/jaes.2017.0011},
   issn = {1549-4950},
   issue = {6},
   journal = {AES: Journal of the Audio Engineering Society},
   month = {6},
   pages = {474-481},
   title = {Music thumbnailing for radio podcasts: A listener evaluation},
   volume = {65},
   year = {2017},
}
@inproceedings{Men2017,
   abstract = {© 2017 IEEE. In recent years, Virtual Reality (VR) applications have become widely available. An increase in popular interest raises questions about the use of the new medium for communication. While there is a wide variety of literature regarding scene transitions in films, novels and computer games, transitions in VR are not yet widely understood. As a medium that requires a high level of immersion [2], transitions are a desirable tool. This poster delineates an experiment studying the impact of transitions on user experience of presence in VR.},
   author = {L Men and N Bryan-Kinns and A S Hassard and Z Ma},
   doi = {10.1109/VR.2017.7892288},
   isbn = {9781509066476},
   journal = {Proceedings - IEEE Virtual Reality},
   month = {4},
   pages = {285-286},
   title = {The impact of transitions on user experience in virtual reality},
   year = {2017},
}
@inproceedings{Milo2017,
   abstract = {© 2017 Copyright held by the owner/author(s). The Aural Fabric is an interactive textile sonic map created to promote engagement in acoustic awareness towards the built environment. It fosters discussions on the aural environment of our cities by allowing users to experience binaural recordings captured during a soundwalk. The touch of the conductive areas embroidered of the surface on the map can be sensed by two capacitive boards stitched on the map. These are externally connected to an embedded computer processing unit, Bela. The recordings can be intuitively mixed together offering exploratory and performative recall of the material collected.},
   author = {A Milo and J D Reiss},
   doi = {10.1145/3123514.3123565},
   isbn = {9781450353731},
   journal = {ACM International Conference Proceeding Series},
   month = {8},
   title = {Aural Fabric: An interactive textile sonic map},
   volume = {Part F131930},
   year = {2017},
}
@inproceedings{MOFFAT2017,
   author = {D J MOFFAT and D Ronan and J D Reiss},
   month = {9},
   title = {Unsupervised Taxonomy of Sound Effects},
   year = {2017},
}
@article{Mohamad2017,
   abstract = {This paper proposes a technique that estimates the locations along the string of the plucking event and the magnetic pickup of an electric guitar based on the autocorrelation of the spectral peaks. To improve accuracy, a method is introduced to flatten the spectrum before applying the autocorrelation function to the spectral peaks. The minimum mean squared error between the autocorrelation of the observed data and the electric guitar model is found in order to estimate the model parameters. The accuracy of the algorithm is tested on various plucking positions on all open strings for each pickup configuration. The accuracy of the proposed method for various plucking dynamics and fret positions is also evaluated. The method yields accurate results: the average absolute errors of the pickup position and plucking point estimates for single pickups are 3.53 and 5.11 mm, respectively, and for mixed pickups are 8.47 and 9.95 mm, respectively. The model can reliably distinguish which pickup configuration is selected using the pickup position estimates. Moreover, the method is robust to changes in plucking dynamics and fret positions.},
   author = {Z Mohamad and S Dixon and C Harte},
   doi = {10.1121/1.5016815},
   issue = {6},
   journal = {J Acoust Soc Am},
   month = {12},
   pages = {3530-3540},
   title = {Pickup position and plucking point estimation on an electric guitar via autocorrelation.},
   volume = {142},
   url = {https://www.ncbi.nlm.nih.gov/pubmed/29289089},
   year = {2017},
}
@inproceedings{Mohamad2017b,
   author = {Z Mohamad and S Dixon and C Harte},
   journal = {Proceedings of the International Conference on Digital Audio Effects (DAFx)},
   pages = {420-426},
   title = {Estimating pickup and plucking positions of guitar tones and chords with audio effects},
   year = {2017},
}
@inproceedings{Mohamad2017c,
   author = {Z Mohamad and S Dixon and C Harte},
   journal = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
   pages = {651-655},
   title = {Pickup position and plucking point estimation on an electric guitar},
   year = {2017},
}
@inproceedings{Morfi2017,
   abstract = {Many approaches have been used in bird species classification from their sound in order to provide labels for the whole of a recording. However, a more precise classification of each bird vocalization would be of great importance to the use and management of sound archives and bird monitoring. In this work, we introduce a technique that using a two step process can first automatically detect all bird vocalizations and then, with the use of âweaklyâ labelled recordings, classify them. Evaluations of our proposed method show that it achieves a correct classification of 75.4% when used in a synthetic dataset.},
   author = {V Morfi and D F STOWELL},
   journal = {International Conference on Acoustics, Speech and Signal Processing (ICASSP2017 )},
   title = {Deductive Refinement of Species Labelling in Weakly Labelled Birdsong Recordings},
   year = {2017},
}
@article{MORO2017,
   author = {G MORO and A MCPHERSON and M SANDLER},
   issn = {1520-8524},
   journal = {Journal of the Acoustical Society of America},
   publisher = {Acoustical Society of America},
   title = {Dynamic behaviour of the keyboard action on the Hammond organ and its perceptual significance},
   year = {2017},
}


@inproceedings{Morreale2017,
   abstract = {This paper reflects on the dynamics and practices of building a maker community around a new hardware platform. We examine the factors promoting the successful uptake of a maker platform from two perspectives: first, we investigate the technical and user experience considerations that users identify as the most important. Second, we explore the specific activities that help attract a community and encourage sustained participation. We present an inductive approach based on the case study of Bela, an embedded platform for creating interactive audio systems. The technical design and community building processes are detailed, culminating in a successful crowdfunding campaign. To further understand the community dynamics, the paper also presents an intensive three-day workshop with eight digital musical instrument designers. From observations and interviews, we reflect on the relationship between the platform and the community and offer suggestions for HCI researchers and practitioners interested in establishing their own maker communities.},
   author = {F Morreale and G Moro and A Chamberlain and S Benford and A MCPHERSON},
   journal = {ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)},
   month = {5},
   title = {Building a Maker Community Around an Open Hardware Platform},
   year = {2017},
}
@article{NAKAMURA2017,
   abstract = {This paper presents a statistical method for use in music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results show that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.},
   author = {E NAKAMURA and K Yoshii and S Dixon},
   doi = {10.1109/TASLP.2017.2722103},
   issn = {2329-9290},
   journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
   month = {6},
   pages = {1},
   title = {Note Value Recognition for Piano Transcription Using Markov Random Fields},
   year = {2017},
}
@inproceedings{Hanlon,
   author = {K O O'Hanlon and S Ewert and J Pauwels and M Sandler},
   month = {6},
   title = {Improved template-based chord recognition using the CRP feature},
   year = {2017},
}
@inproceedings{Olowe2017,
   abstract = {© 2017 Copyright held by the owner/author(s). In this paper, we describe the conceptual design and technical implementation of an audiovisual system whereby multitrack audio is used to generate visualizations in real time. We discuss our motivation within the context of audiovisual practice and present the outcomes of studies conducted to outline design requirements. We then describe the audio and visual components of our multitrack visualization model, and specific parts of the graphical user interface (GUI) which focus on mapping as the primary mechanism to facilitate live multitrack audiovisual performance.},
   author = {I Olowe and M Grierson and M Barthet},
   doi = {10.1145/3123514.3123561},
   isbn = {9781450353731},
   journal = {ACM International Conference Proceeding Series},
   month = {8},
   title = {FEATUR.UX.AV: A live sound visualization system using multitrack audio},
   volume = {Part F131930},
   year = {2017},
}
@inproceedings{Olowe2017b,
   abstract = {© 2017 Copyright held by the owner/author(s). In this paper, we identify design requirements for a screen-based system that enables live sound visualization using multitrack audio. Our mixed methodology is grounded in user-centered design and involved a review of the literature to assess the state-of-the-art of Video Jockeying (VJing), and two online surveys to canvas practices within the audiovisual community and gain practical and aspirational awareness on the subject. We review ten studies about VJ practice and culture and human computer interaction topics within live performance. Results from the first survey, completed by 22 participants, were analysed to identify general practices, mapping preferences, and impressions about multitrack audio and audio-content feature extraction. A second complementary survey was designed to probe about specific implications of performing with a system that facilitates live visual performance using multitrack audio. Analyses from 29 participants' self-reports highlight that the creation of audiovisual content is a multivariate and subjective process and help define where multitrack audio, audio-content extraction, and live mapping could fit within. We analyze the findings and discuss how they can inform a design for our system.},
   author = {I Olowe and M Grierson and M Barthet},
   doi = {10.1145/3123514.3123527},
   isbn = {9781450353731},
   journal = {ACM International Conference Proceeding Series},
   month = {8},
   title = {User requirements for live sound visualization system using multitrack audio},
   volume = {Part F131930},
   year = {2017},
}
@inproceedings{Panteli2017,
   abstract = {© 2017 IEEE. In this paper we focus on the characterization of singing styles in world music. We develop a set of contour features capturing pitch structure and melodic embellishments. Using these features we train a binary classifier to distinguish vocal from non-vocal contours and learn a dictionary of singing style elements. Each contour is mapped to the dictionary elements and each recording is summarized as the histogram of its contour mappings. We use K-means clustering on the recording representations as a proxy for singing style similarity. We observe clusters distinguished by characteristic uses of singing techniques such as vibrato and melisma. Recordings that are clustered together are often from neighbouring countries or exhibit aspects of language and cultural proximity. Studying singing particularities in this comparative manner can contribute to understanding the interaction and exchange between world music styles.},
   author = {M Panteli and R Bittner and J P Bello and S Dixon},
   doi = {10.1109/ICASSP.2017.7952233},
   isbn = {9781509041176},
   issn = {1520-6149},
   journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
   month = {6},
   pages = {636-640},
   title = {Towards the characterization of singing styles in world music},
   year = {2017},
}
@article{PANTELI2017b,
   abstract = {The comparative analysis of world music cultures has been the focus of several ethnomusicological studies in the last century. With the advances of Music Information Retrieval and the increased accessibility of sound archives, large-scale analysis of world music with computational tools is today feasible. We investigate music similarity in a corpus of 8200 recordings of folk and traditional music from 137 countries around the world. In particular, we aim to identify music recordings that are most distinct compared to the rest of our corpus. We refer to these recordings as ‘outliers’. We use signal processing tools to extract music information from audio recordings, data mining to quantify similarity and detect outliers, and spatial statistics to account for geographical correlation. Our findings suggest that Botswana is the country with the most distinct recordings in the corpus and China is the country with the most distinct recordings when considering spatial correlation. Our analysis includes a comparison of musical attributes and styles that contribute to the ‘uniqueness’ of the music of each country.},
   author = {M PANTELI and E BENETOS and S DIXON},
   doi = {10.1371/journal.pone.0189399},
   issn = {1932-6203},
   issue = {12},
   journal = {PLoS ONE},
   month = {12},
   pages = {1-28},
   publisher = {Public Library of Science (PLoS)},
   title = {A computational study on outliers in world music},
   volume = {12},
   url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0189399},
   year = {2017},
}
@article{Pearce2017,
   author = {M Pearce and D Müllensiefen},
   doi = {10.1080/09298215.2017.1305419},
   issn = {0929-8215},
   issue = {2},
   journal = {Journal of New Music Research},
   month = {4},
   note = {peerreview_statement: The publishing and review policy for this title is described in its Aims & Scope.
aims_and_scope_url: http://www.tandfonline.com/action/journalInformation?show=aimsScope&journalCode=nnmr20},
   pages = {135-155},
   title = {Compression-based Modelling of Musical Similarity Perception},
   volume = {46},
   year = {2017},
}
@inproceedings{Pigrem2017,
   author = {Jon Pigrem and Mathieu Barthet},
   journal = {Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences},
   pages = {43},
   title = {Datascaping: Data Sonification as a Narrative Device in Soundscape Composition},
   year = {2017},
}
@inproceedings{Quinton2017,
   author = {E Quinton and S Dixon and M Sandler and K O O'Hanlon},
   month = {6},
   title = {TRACKING METRICAL STRUCTURE CHANGES WITH SPARSE-NMF},
   year = {2017},
}
@article{REISS2017,
   author = {J REISS},
   issn = {1549-4950},
   journal = {Journal of the Audio Engineering Society},
   month = {2},
   publisher = {Audio Engineering Society},
   title = {User Preference on Artificial Reverberation and Delay Time Parameters},
   year = {2017},
}
@article{REISS2017b,
   author = {J REISS and B De Man},
   doi = {10.17743/jaes.2016.0062},
   issn = {1549-4950},
   journal = {Journal of the Audio Engineering Society},
   month = {2},
   publisher = {Audio Engineering Society},
   title = {Perceptual evaluation and analysis of reverberation in multitrack music production},
   year = {2017},
}
@inproceedings{Russell2017,
   abstract = {In this paper, we investigate the memory properties of two popular gated units: long short term memory (LSTM) and gated recurrent units (GRU), which have been used in recurrent neural networks (RNN) to achieve state-of-the-art performance on several machine learning tasks. We propose five basic tasks for isolating and examining specific capabilities relating to the implementation of memory. Results show that (i) both types of gated unit perform less reliably than standard RNN units on tasks testing fixed delay recall, (ii) the reliability of stochastic gradient descent decreases as network complexity increases, and (iii) gated units are found to perform better than standard RNNs on tasks that require values to be stored in memory and updated conditionally upon input to the network. Task performance is found to be surprisingly independent of network depth (number of layers) and connection architecture. Finally, visualisations of the solutions found by these networks are presented and explored, exposing for the first time how logic operations are implemented by individual gated cells and small groups of these cells.},
   author = {A J Russell and E Benetos and A S d'Avila Garcez},
   journal = {International Joint Conference on Neural Networks (IJCNN 2017)},
   month = {5},
   title = {On the Memory Properties of Recurrent Neural Models},
   year = {2017},
}
@inproceedings{Schramm2017,
   abstract = {This work presents a spectrogram factorisation method applied to automatic music transcription of a cappella performances with multiple singers. A variable-Q transform representation of the audio spectrogram is factorised with the help of a 6-dimensional sparse dictionary which contains spectral templates of vowel vocalizations. A post-processing step is proposed to remove false positive pitch detections through a binary classifier, where overtone-based features are used as input into this step. Preliminary experiments have shown promising multi-pitch detection results when applied to audio recordings of Bach Chorales and Barbershop music. Comparisons made with alternative methods have shown that our approach increases the number of true positive pitch detections while the post-processing step keeps the number of false positives lower than those measured in comparative approaches.},
   author = {R Schramm and E Benetos},
   doi = {10.17743/aesconf.2017.978-1-942220-15-2},
   month = {6},
   publisher = {Audio Engineering Society},
   title = {Automatic Transcription of a Cappella Recordings from Multiple Singers},
   url = {http://professor.ufrgs.br/rschramm},
   year = {2017},
}
@inproceedings{Schramm2017b,
   abstract = {This paper presents a multi-pitch detection and voice assignment method applied to audio recordings containing a cappella performances with multiple singers. A novel approach combining an acoustic model for multi-pitch detection and a music language model for voice separation and assignment is proposed. The acoustic model is a spectrogram factorization process based on Probabilistic Latent Component Analysis (PLCA), driven by a 6-dimensional dictionary with pre-learned spectral templates. The voice separation component is based on hidden Markov models that use musicological assumptions. By integrating the models, the system can detect multiple concurrent pitches in vocal music and assign each detected pitch to a specific voice corresponding to a voice type such as soprano, alto, tenor or bass (SATB). This work focuses on four-part compositions, and evaluations on recordings of Bach Chorales and Barbershop quartets show that our integrated approach achieves an F-measure of over 70% for frame-based multi-pitch detection and over 45% for four-voice assignment.},
   author = {R Schramm and A McLeod and M Steedman and E Benetos},
   month = {10},
   pages = {552-559},
   publisher = {ISMIR},
   title = {Multi-pitch detection and voice assignment for a cappella recordings of multiple singers},
   url = {https://ismir2017.smcnus.org/},
   year = {2017},
}
@article{Sears2017,
   author = {D R W Sears and M T PEARCE and W E Caplin and S McAdams},
   doi = {10.1080/09298215.2017.1367010},
   issn = {0929-8215},
   journal = {Journal of New Music Research},
   month = {9},
   publisher = {Taylor & Francis (Routledge)},
   title = {Simulating melodic and harmonic expectations for tonal cadences using probabilistic models},
   year = {2017},
}
@inproceedings{SELFRIDGE2017,
   abstract = {Sword sounds are synthesised by physical models in real- time. A number of compact sound sources are used along the length of the sword which replicate the swoosh sound when swung through the air. Listening tests are carried out which reveal a model with reduced physics is perceived as more authentic. The model is further developed to be controlled by a Wii Controller and successfully extended to include sounds of a baseball bat and golf club.},
   author = {R SELFRIDGE and D Moffat and J Reiss},
   month = {7},
   title = {REAL-TIME PHYSICAL MODEL FOR SYNTHESIS OF SWORD SWING SOUNDS},
   year = {2017},
}
@inproceedings{SELFRIDGE2017b,
   abstract = {A real-time physical sound synthesis model of an Aeolian harp is presented. The model uses semi- empirical fluid dynamics equations to inform its operation, providing suitable parameters for users to interact. A basic wind model is included as well as an interface allowing user adjustable param- eters. Sounds generated by the model were subject to objective measurements against real-world recordings, which showed that many of the physical properties of the harp were replicated in our model, but a possible link between harmonics and vibration amplitude was not. A perceptual test was performed, where participants were asked to rate sounds in terms of how plausible they were in comparison with spectral modelling synthesis and recorded Aeolian Harp samples. Evaluation showed that our model performed as well as an alternative non-physical synthesis method, but was not as authentic as actual recorded samples.},
   author = {R SELFRIDGE and D Moffat and J Reiss and E Avital},
   month = {7},
   title = {Real-time physical model of an Aeolian harp},
   year = {2017},
}
@inproceedings{Selfridge2017c,
   abstract = {© 2017 Copyright held by the owner/author(s). A real-time sound synthesis model for propeller sounds is presented. Equations obtained from fluid dynamics and aerodynamics research are utilised to produce authentic propeller-powered aircraft sounds. The result is a physical model in which the geometries of the objects involved are used in sound synthesis calculations. The model operates in real-time making it ideal for integration within a game or virtual reality environment. Comparison with real propeller-powered aircraft sounds indicates that some aspects of real recordings are not replicated by our model. Listening tests suggest that our model performs as well as another synthesis method but is not as plausible as a real recording.},
   author = {R Selfridge and D Moffat and J D Reiss},
   doi = {10.1145/3123514.3123524},
   isbn = {9781450353731},
   journal = {ACM International Conference Proceeding Series},
   month = {8},
   title = {Physically derived sound synthesis model of a propeller},
   volume = {Part F131930},
   year = {2017},
}
@article{Selfridge2017d,
   abstract = {© 2017 by the authors. A real-time physically-derived sound synthesis model is presented that replicates the sounds generated as an object swings through the air. Equations obtained from fluid dynamics are used to determine the sounds generated while exposing practical parameters for a user or game engine to vary. Listening tests reveal that for the majority of objects modelled, participants rated the sounds from our model as plausible as actual recordings. The sword sound effect performed worse than others, and it is speculated that one cause may be linked to the difference between expectations of a sound and the actual sound for a given object.},
   author = {R Selfridge and D Moffat and J D Reiss},
   doi = {10.3390/app7111177},
   issue = {11},
   journal = {Applied Sciences (Switzerland)},
   month = {11},
   title = {Sound synthesis of objects swinging through air using physical models},
   volume = {7},
   year = {2017},
}
@inproceedings{Song2017,
   abstract = {© 2017 IEEE. The broadcasting industry has recently begun to adopt statistical multiplexing based network platform in their workflow to support professional live audio/video (AV) transmission instead of the Time Division Multiplexing (TDM) based system. These audio-over-packet switched systems require a carefully designed and managed network to ensure key quality measures of the real-time (RT) media, such as low jitter and low latency. Often the best effort traffic or different types of media are still physically or logically segregated from these dedicated systems, or require large redundant links. The proposed Flexilink architecture is an alternative that combines both circuit switched and best effort features. However, there is no research evaluation that shows the actual performance of this proposed architecture. In this paper, we give a simulation based study and critical evaluation of the performance of the Flexilink network. The simulation results show that Flexilink has a better and more stable RT performance when compared with both Ethernet and priority queueing networks, especially when given a burst of traffic and/or multiple RT traffic sources. In addition, Unlike other networking protocols, jitter in Flexilink is below the audible threshold.},
   author = {Y Song and Y Wang and P Bull and J D Reiss},
   doi = {10.1109/AINA.2017.80},
   isbn = {9781509060283},
   issn = {1550-445X},
   journal = {Proceedings - International Conference on Advanced Information Networking and Applications, AINA},
   month = {5},
   pages = {23-30},
   title = {Performance evaluation of a new flexible time division multiplexing protocol on mixed traffic types},
   year = {2017},
}
@inproceedings{STEWART2017,
   abstract = {is paper evaluates three electronic textile (e-textile) stretch sensors commonly constructed for bespoke applications: two variations of fabric knit with a stainless steel and polyester yarn, and knit fabric coated with a conductive polymer. Two versions of the knit stainless steel and polyester yarn sensor, one hand and one machine knit, are evaluated. All of the materials used in the construction of the sensors are accessible to designers and engineers, and are commonly used in wearable technology projects, particularly for arts performance. However, the properties of each sensor have not before been formally analysed. We evaluate the sensors’ performance when being stretched and released.},
   author = {R L STEWART and S Skach},
   doi = {10.1145/3077981.3078043},
   month = {6},
   title = {Initial Investigations into Characterizing DIY E-Textile Stretch Sensors},
   year = {2017},
}
@article{Stowell2017,
   abstract = {We introduce a novel approach to studying animal behaviour and the context in which it occurs, through the use of microphone backpacks carried on the backs of individual free-flying birds. These sensors are increasingly used by animal behaviour researchers to study individual vocalisations of freely behaving animals, even in the field. However such devices may record more than an animals vocal behaviour, and have the potential to be used for investigating specific activities (movement) and context (background) within which vocalisations occur. To facilitate this approach, we investigate the automatic annotation of such recordings through two different sound scene analysis paradigms: a scene-classification method using feature learning, and an event-detection method using probabilistic latent component analysis (PLCA). We analyse recordings made with Eurasian jackdaws (Corvus monedula) in both captive and field settings. Results are comparable with the state of the art in sound scene analysis; we find that the current recognition quality level enables scalable automatic annotation of audio logger data, given partial annotation, but also find that individual differences between animals and/or their backpacks limit the generalisation from one individual to another. we consider the interrelation of 'scenes' and 'events' in this particular task, and issues of temporal resolution.},
   author = {D Stowell and E Benetos and L F Gill},
   doi = {10.1109/TASLP.2017.2690565},
   issn = {2329-9290},
   issue = {6},
   journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
   month = {5},
   pages = {1193-1206},
   publisher = {IEEE},
   title = {On-bird Sound Recordings: Automatic Acoustic Recognition of Activities and Contexts},
   volume = {25},
   url = {http://arxiv.org/abs/1612.05489v1},
   year = {2017},
}
@article{STURM2017,
   author = {B L T STURM and O Ben-Tal},
   journal = {Journal of Creative Music Systems},
   title = {Bringing the models back to music practice: The evaluation of deep learning approaches to music transcription modelling and generation},
   year = {2017},
}
@inproceedings{Valero2017,
   author = {J J Valero-Mas and E Benetos and J M IÃ±esta},
   journal = {2017 AES International Conference on Semantic Audio},
   month = {6},
   publisher = {Audio Engineering Society},
   title = {Assessing the Relevance of Onset Information for Note Tracking in Piano Music Transcription},
   url = {http://www.aes.org/conferences/2017/semantic/},
   year = {2017},
}
@article{Brecht2017,
   abstract = {Enculturation is known to shape the perception of meter in music but this is not explicitly accounted for by current cognitive models of meter perception. We hypothesize that the induction of meter is a result of predictive coding: interpreting onsets in a rhythm relative to a periodic meter facilitates prediction of future onsets. Such prediction, we hypothesize, is based on previous exposure to rhythms. As such, predictive coding provides a possible explanation for the way meter perception is shaped by the cultural environment. Based on this hypothesis, we present a probabilistic model of meter perception that uses statistical properties of the relation between rhythm and meter to infer meter from quantized rhythms. We show that our model can successfully predict annotated time signatures from quantized rhythmic patterns derived from folk melodies. Furthermore, we show that by inferring meter, our model improves prediction of the onsets of future events compared to a similar probabilistic model that does not infer meter. Finally, as a proof of concept, we demonstrate how our model can be used in a simulation of enculturation. From the results of this simulation, we derive a class of rhythms that are likely to be interpreted differently by enculturated listeners with different histories of exposure to rhythms.},
   author = {B van der Weij and M T Pearce and H Honing},
   doi = {10.3389/fpsyg.2017.00824},
   journal = {Front Psychol},
   month = {5},
   pages = {824},
   title = {A Probabilistic Model of Meter Perception: Simulating Enculturation.},
   volume = {8},
   url = {https://www.ncbi.nlm.nih.gov/pubmed/28588533},
   year = {2017},
}
@article{WANG2017,
   author = {S WANG and SEBASTIAN Ewert and SIMON Dixon},
   doi = {10.1109/TASLP.2017.2724203},
   journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
   title = {Identifying Missing and Extra Notes in Piano Recordings Using Score-Informed Dictionary Learning},
   year = {2017},
}
@inproceedings{White2017,
   abstract = {© 2017 Copyright held by the owner/author(s). In this paper we present a novel system for performative interaction with an archive of public domain music recordings. The system uses real-time query by vocalization to retrieve sounds extracted from chart hit singles of the 1960s. This enables the user, or performer, to generate a cascade of archival echoes from vocalisations. The system was developed for a series of music workshops, held as part of a two art projects centered around the reuse and repurposing of archive recordings. As such the design decisions were shaped by the conceptual framework of the artists and intended audience. Here we outline the context and background of the art projects, describe the query by vocalisation system and discuss the workshops, where the artists invited amateur musicians to use the system to develop a public performance.},
   author = {B White and A Mehrabi and M B Sandler},
   doi = {10.1145/3123514.3123546},
   isbn = {9781450353731},
   journal = {ACM International Conference Proceeding Series},
   month = {8},
   title = {An Archival Echo: Recalling the public domain through real-time query by vocalisation},
   volume = {Part F131930},
   year = {2017},
}
@inproceedings{Wilkinson2017,
   abstract = {Latent force models are a Bayesian learning technique that combine physical knowledge with dimensionality reduction - sets of coupled differential equations are modelled via shared dependence on a low-dimensional latent space. Analogously, modal sound synthesis is a technique that links physical knowledge about the vibration of objects to acoustic phenomena that can be observed in data. We apply latent force modelling to sinusoidal models of audio recordings, simultaneously inferring modal synthesis parameters (stiffness and damping) and the excitation or contact force required to reproduce the behaviour of the observed vibrational modes. Exposing this latent excitation function to the user constitutes a controllable synthesis method that runs in real time and enables sound morphing through interpolation of learnt parameters.},
   author = {W J Wilkinson and J D Reiss and D Stowell},
   journal = {DAFx 2017 - Proceedings of the 20th International Conference on Digital Audio Effects},
   month = {10},
   pages = {56-63},
   title = {Latent force models for sound: Learning modal synthesis parameters and excitation functions from audio recordings},
   year = {2017},
}
@article{Wu2017,
   abstract = {© 1994-2012 IEEE. Most contemporary Western performing arts practices restrict creative interactions from audiences. Open Symphony is designed to explore audience-performer interaction in live music performances, assisted by digital technology. Audiences can conduct improvising performers by voting for various musical 'modes.' Technological components include a web-based mobile application, a visual client displaying generated symbolic scores, and a server service for the exchange of creative data. The interaction model, app, and visualization were designed through an iterative participatory design process. The system was experienced by about 120 audience and performer participants (35 completed surveys) in controlled (lab) and real-world settings. Feedback on usability and user experience was overall positive, and live interactions demonstrate significant levels of audience creative engagement. The authors identified further design challenges around audience sense of control, learnability, and compositional structure. This article is part of a special issue on multimedia for enriched music.},
   author = {Y Wu and L Zhang and N Bryan-Kinns and M Barthet},
   doi = {10.1109/MMUL.2017.19},
   issn = {1070-986X},
   issue = {1},
   journal = {IEEE Multimedia},
   month = {2},
   pages = {48-62},
   title = {Open Symphony: Creative Participation for Audiences of Live Music Performances},
   volume = {24},
   year = {2017},
}
@inproceedings{Wu2017b,
   abstract = {© 2017 ACM. This paper reports on a study which sets out to examine the process of creative engagement of individual non-musicians when interacting with interactive musical systems (IMSs), and to identify features of IMSs that may support non-musician's creative engagement in music making. In order to creatively engage novices, we aimed to design IMSs which would combine the intuitive interaction of 'sound toys' with the rich and complex music possibilities offered by IMSs to support individual learning and creative processes. Two IMSs were designed and built and an empirical user study was conducted of them. The study used a multi-layered methodology, which combined interviews and recordings of user interactions. Analysis of participants' behaviour with the IMSs led to the identification of interaction patterns, which were used to identify a threestep framework ('learn', 'exploration','creation') of creative engagement with the musical interfaces, and to inform the development of implications for design for supporting creative engagement with IMSs. Key implications for design identified in this study are to support novices to: learn the sound; play live; catalyze insights; and scaffold composition. The basic study methodology of this paper also offers a contribution to methods for evaluating designs by combining recording user interactions with interviews.},
   author = {Y Wu and N Bryan-Kinns},
   doi = {10.1145/3059454.3059457},
   isbn = {9781450344036},
   journal = {C and C 2017 - Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition},
   month = {6},
   pages = {275-286},
   title = {Supporting non-musicians' creative engagement with musical interfaces},
   year = {2017},
}
@article{YANG2017,
   author = {L YANG and SAYID-KHALID Rajab and E Chew},
   issn = {1745-9745},
   journal = {Journal of Mathematics and Music},
   month = {3},
   publisher = {Taylor &amp; Francis: STM, Behavioural Science and Public Health Titles},
   title = {Filter Diagonalisation Method for Music Signal Analysis: Frame-wise Vibrato Detection and Estimation},
   year = {2017},
}
@inproceedings{Yang2017b,
   abstract = {© 2017 IEEE. Transcribing the singing voice into music notes is challenging due to pitch fluctuations such as portamenti and vibratos. This paper presents a probabilistic transcription method for monophonic sung melodies that explicitly accounts for these local pitch fluctuations. In the hierarchical Hidden Markov Model (HMM), an upper-level ergodic HMM handles the transitions between notes, and a lower-level left-to-right HMM handles the intra- and inter-note pitch fluctuations. The lower-level HMM employs the pitch dynamic model, which explicitly expresses the pitch curve characteristics as the observation likelihood over ∞ 0 and Δ∞ 0 using a compact parametric distribution. A histogram-based tuning frequency estimation method, and some post-processing heuristics to separate merged notes and to allocate spuriously detected short notes, improve the note recognition performance. With model parameters that support intuitions about singing behavior, the proposed method obtained encouraging results when evaluated on a published monophonic sung melody dataset, and compared with state-of-the-art methods.},
   author = {L Yang and A Maezawa and J B L Smith and E Chew},
   doi = {10.1109/ICASSP.2017.7952166},
   isbn = {9781509041176},
   issn = {1520-6149},
   journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
   month = {6},
   pages = {301-305},
   title = {Probabilistic transcription of sung melody using a pitch dynamic model},
   year = {2017},
}
@inproceedings{Ycart2017,
   abstract = {Neural networks, and especially long short-term memory networks (LSTM), have become increasingly popular for sequence modelling, be it in text, speech, or music. In this paper, we investigate the predictive power of simple LSTM networks for polyphonic MIDI sequences, using an empirical approach. Such systems can then be used as a music language model which, combined with an acoustic model, can improve automatic music transcription (AMT) performance. As a first step, we experiment with synthetic MIDI data, and we compare the results obtained in various settings, throughout the training process. In particular, we compare the use of a fixed sample rate against a musically-relevant sample rate. We test this system both on synthetic and real MIDI data. Results are compared in terms of note prediction accuracy. We show that the higher the sample rate is, the better the prediction is, because self transitions are more frequent. We suggest that for AMT, a musically-relevant sample rate is crucial in order to model note transitions, beyond a simple smoothing effect.},
   author = {A Ycart and E Benetos},
   month = {10},
   pages = {421-427},
   publisher = {ISMIR},
   title = {A study on LSTM networks for polyphonic music sequence modelling},
   url = {http://www.eecs.qmul.ac.uk/~ay304/},
   year = {2017},
}
@article{Zacharakis2017,
   author = {A Zacharakis and M Terrell and A R Simpson and K Pastiadis and J Reiss},
   doi = {10.3813/AAA.919057},
   issn = {1610-1928},
   issue = {2},
   journal = {Acta Acustica united with Acustica},
   month = {3},
   pages = {288-298},
   title = {Rearrangement of Timbre Space Due To Background Noise: Behavioural Evidence and Acoustic Correlates},
   volume = {103},
   year = {2017},
}
@inproceedings{Zhu2017,
   abstract = {© 2016 IEEE. Multistage filter design is a complex multidimensional optimisation problem. The formulae for optimal design generally yield non-integer real numbers for the sample-rate-changing factors of multiple stages. Approaches yielding useful integer results have high computational cost and do not consider important multistage filter design properties. We have developed a simplified algorithm for directly searching the optimal integer results. Considering the most useful practical design parameters, optimal results can be approximated with a limited number of sets for any designs satisfying certain constraints, with negligible costs. This vastly simplifies the complexity of the problem.},
   author = {X Zhu and Y Wang and W Hu and J D Reiss},
   doi = {10.1109/ICDSP.2016.7868581},
   isbn = {9781509041657},
   journal = {International Conference on Digital Signal Processing, DSP},
   month = {3},
   pages = {370-374},
   title = {Practical considerations on optimising multistage decimation and interpolation processes},
   year = {2017},
}
